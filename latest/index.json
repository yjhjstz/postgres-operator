[
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/configuration/compatibility/",
	"title": "Compatibility Requirements",
	"tags": [],
	"description": "",
	"content": " Container Dependencies The Operator depends on the Crunchy Containers and there are version dependencies between the two projects.\n   Operator Release Container Release     4.0.0 2.4.0   3.5.2 2.3.1    Features sometimes are added into the underlying Crunchy Containers to support upstream features in the Operator thus dictating a dependency between the two projects at a specific version level.\nOperating Systems The Operator is developed on both Centos 7 and RHEL 7 operating systems. The underlying containers are designed to use either Centos 7 or RHEL 7 as the base container image.\nOther Linux variants are possible but are not supported at this time.\nKubernetes Distributions The Operator is designed and tested on Kubernetes and Openshift Container Platform.\nStorage The Operator is designed to support HostPath, NFS, and Storage Classes for persistence. The Operator does not currently include code specific to a particular storage vendor.\nReleases The Operator is released on a quarterly basis often to coincide with Postgres releases.\nThere are pre-release and or minor bug fix releases created on an as-needed basis.\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/install-with-ansible/",
	"title": "Install Operator Using Ansible",
	"tags": [],
	"description": "",
	"content": " Crunchy Data PostgreSQL Operator Playbooks The Crunchy Data PostgreSQL Operator Playbooks contain Ansible roles for installing and managing the Crunchy Data PostgreSQL Operator.\nFeatures The playbooks provided allow users to:\n install PostgreSQL Operator on Kubernetes and OpenShift install PostgreSQL Operator from a Linux, Mac or Windows(Ubuntu subsystem)host generate TLS certificates required by the PostgreSQL Operator configure PostgreSQL Operator settings from a single inventory file support a variety of deployment models  Resources  Ansible Crunchy Data Crunchy Data PostgreSQL Operator Documentation Crunchy Data PostgreSQL Operator Project  "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/install-pgo-client/",
	"title": "Install `pgo` Client",
	"tags": [],
	"description": "",
	"content": " Install the Postgres Operator (pgo) Client The following will install and configure the pgo client on all systems. For the purpose of these instructions it\u0026rsquo;s assumed that the Crunchy PostgreSQL Operator is already deployed.\nPrerequisites  For Kubernetes deployments: kubectl configured to communicate with Kubernetes For OpenShift deployments: oc configured to communicate with OpenShift  The Crunchy Postgres Operator als requires the following in order to authenticate with the apiserver:\n Client CA Certificate Client TLS Certificate Client Key pgouser file containing \u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;  All of the requirements above should be obtained from an administrator who installed the Crunchy PostgreSQL Operator.\nLinux and MacOS The following will setup the pgo client to be used on a Windows system.\nInstalling the Client First, download the pgo.exe client from the GitHub official releases.\nNext, install pgo in /usr/local/bin by running the following:\nsudo mv /PATH/TO/pgo /usr/local/bin/pgo sudo chmod +x /usr/local/bin/pgo Verify the pgo.exe client is accessible by running the following in the terminal:\npgo --help Configuring Client TLS With the client TLS requirements satisfied we can setup pgo to use them.\nFirst, create a directory to hold these files by running the following command:\nmkdir ${HOME?}/.pgo chmod 700 ${HOME?}/.pgo Next, copy the certificates to this new directory:\ncp /PATH/TO/client.crt ${HOME?}/.pgo/client.crt \u0026amp;\u0026amp; chmod 600 ${HOME?}/.pgo/client.crt cp /PATH/TO/client.pem ${HOME?}/.pgo/client.pem \u0026amp;\u0026amp; chmod 400 ${HOME?}/.pgo/client.pem Finally, set the following environment variables to point to the client TLS files:\ncat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; ${HOME?}/.bashrc export PGO_CA_CERT=\u0026#34;${HOME?}/.pgo/client.crt\u0026#34; export PGO_CLIENT_CERT=\u0026#34;${HOME?}/.pgo/client.crt\u0026#34; export PGO_CLIENT_KEY=\u0026#34;${HOME?}/.pgo/client.pem\u0026#34; EOF Apply those changes to the current session by running:\nsource ~/.bashrc Configuring pgouser The pgouser file contains the username and password used for authentication with the Crunchy PostgreSQL Operator.\nTo setup the pgouser file, run the following:\necho \u0026#34;\u0026lt;USERNAME_HERE\u0026gt;:\u0026lt;PASSWORD_HERE\u0026gt;\u0026#34; \u0026gt; ${HOME?}/.pgo/pgousercat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; ${HOME?}/.bashrc export PGOUSER=\u0026#34;${HOME?}/.pgo/pgouser\u0026#34; EOF Apply those changes to the current session by running:\nsource ${HOME?}/.bashrc Configuring the API Server URL If the Crunchy PostgreSQL Operator is not accessible outside of the cluster, it\u0026rsquo;s required to setup a port-forward tunnel using the kubectl or oc binary.\nIn a separate terminal we need to setup a port forward to the Crunchy PostgreSQL Operator to ensure connection can be made outside of the cluster:\n# If deployed to Kubernetes kubectl port-forward \u0026lt;OPERATOR_POD_NAME\u0026gt; -n \u0026lt;OPERATOR_NAMESPACE\u0026gt; 8443:8443 # If deployed to OpenShift oc port-forward \u0026lt;OPERATOR_POD_NAME\u0026gt; -n \u0026lt;OPERATOR_NAMESPACE\u0026gt; 8443:8443 Note: the port-forward will be required for the duration of pgo usage.\nNext, set the following environment variable to configure the API server address:\ncat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; ${HOME?}/.bashrc export PGO_APISERVER_URL=\u0026#34;https://\u0026lt;IP_OF_OPERATOR_API\u0026gt;:8443\u0026#34; EOF Note: if port-forward is being used, the IP of the Operator API is 127.0.0.1\nApply those changes to the current session by running:\nsource ${HOME?}/.bashrc Windows The following will setup the pgo client to be used on a Windows system.\nInstalling the Client First, download the pgo.exe client from the GitHub official releases.\nNext, create a directory for pgo using the following:\n Left click the Start button in the bottom left corner of the taskbar Type cmd to search for Command Prompt Right click the Command Prompt application and click \u0026ldquo;Run as administrator\u0026rdquo; Enter the following command: mkdir \u0026quot;%ProgramFiles%\\postgres-operator\u0026quot;  Within the same terminal copy the pgo.exe binary to the directory created above using the following command:\ncopy %HOMEPATH%\\Downloads\\pgo.exe \u0026#34;%ProgramFiles%\\postgres-operator\u0026#34; Finally, add pgo.exe to the system path by running the following command in the terminal:\nsetx path \u0026#34;%path%;C:\\Program Files\\postgres-operator\u0026#34; Verify the pgo.exe client is accessible by running the following in the terminal:\npgo --help Configuring Client TLS With the client TLS requirements satisfied we can setup pgo to use them.\nFirst, create a directory to hold these files using the following:\n Left click the Start button in the bottom left corner of the taskbar Type cmd to search for Command Prompt Right click the Command Prompt application and click \u0026ldquo;Run as administrator\u0026rdquo; Enter the following command: mkdir \u0026quot;%HOMEPATH%\\pgo\u0026quot;  Next, copy the certificates to this new directory:\ncopy \\PATH\\TO\\client.crt \u0026#34;%HOMEPATH%\\pgo\u0026#34; copy \\PATH\\TO\\client.pem \u0026#34;%HOMEPATH%\\pgo\u0026#34; Finally, set the following environment variables to point to the client TLS files:\nsetx PGO_CA_CERT \u0026#34;%HOMEPATH%\\pgo\\client.crt\u0026#34; setx PGO_CLIENT_CERT \u0026#34;%HOMEPATH%\\pgo\\client.crt\u0026#34; setx PGO_CLIENT_KEY \u0026#34;%HOMEPATH%\\pgo\\client.pem\u0026#34; Configuring pgouser The pgouser file contains the username and password used for authentication with the Crunchy PostgreSQL Operator.\nTo setup the pgouser file, run the following:\n Left click the Start button in the bottom left corner of the taskbar Type cmd to search for Command Prompt Right click the Command Prompt application and click \u0026ldquo;Run as administrator\u0026rdquo; Enter the following command: echo USERNAME_HERE:PASSWORD_HERE \u0026gt; %HOMEPATH%\\pgo\\pgouser  Finally, set the following environment variable to point to the pgouser file:\nsetx PGOUSER \u0026quot;%HOMEPATH%\\pgo\\pgouser\u0026quot;  Configuring the API Server URL If the Crunchy PostgreSQL Operator is not accessible outside of the cluster, it\u0026rsquo;s required to setup a port-forward tunnel using the kubectl or oc binary.\nIn a separate terminal we need to setup a port forward to the Crunchy PostgreSQL Operator to ensure connection can be made outside of the cluster:\n# If deployed to Kubernetes kubectl port-forward \u0026lt;OPERATOR_POD_NAME\u0026gt; -n \u0026lt;OPERATOR_NAMESPACE\u0026gt; 8443:8443 # If deployed to OpenShift oc port-forward \u0026lt;OPERATOR_POD_NAME\u0026gt; -n \u0026lt;OPERATOR_NAMESPACE\u0026gt; 8443:8443 Note: the port-forward will be required for the duration of pgo usage.\nNext, set the following environment variable to configure the API server address:\n Left click the Start button in the bottom left corner of the taskbar Type cmd to search for Command Prompt Right click the Command Prompt application and click \u0026ldquo;Run as administrator\u0026rdquo; Enter the following command: setx PGO_APISERVER_URL \u0026quot;https://\u0026lt;IP_OF_OPERATOR_API\u0026gt;:8443\u0026quot;  Note: if port-forward is being used, the IP of the Operator API is 127.0.0.1   Verify the Client Installation After completing all of the steps above we can verify pgo is configured properly by simply running the following:\npgo version If the above command outputs versions of both the client and API server, the Crunchy PostgreSQL Operator client has been installed successfully.\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/configuration/configuration/",
	"title": "Configuration Resources",
	"tags": [],
	"description": "",
	"content": " The operator is template-driven; this makes it simple to configure both the client and the operator.\nconf Directory The Operator is configured with a collection of files found in the conf directory. These configuration files are deployed to your Kubernetes cluster when the Operator is deployed. Changes made to any of these configuration files currently require a redeployment of the Operator on the Kubernetes cluster.\nThe server components of the Operator include Role Based Access Control resources which need to be created a single time by a Kubernetes cluster-admin user. See the Installation section for details on installing a Postgres Operator server.\nThe configuration files used by the Operator are found in 2 places: * the pgo-config ConfigMap in the namespace the Operator is running in * or, a copy of the configuration files are also included by default into the Operator container images themselves to support a very simplistic deployment of the Operator\nIf the pgo-config ConfigMap is not found by the Operator, it will use the configuration files that are included in the Operator container images.\nThe container included set of configuration files use the most basic setting values and the image versions of the Operator itself with the latest Crunchy Container image versions. The storage configurations are determined by using the default storage class on the system you are deploying the Operator into, the default storage class is one that is labeled as follows:\npgo-default-sc=true  If no storage class has that label, then the first storage class found on the system will be used. If no storage class is found on the system, the containers will not run and produce an error in the log.\nconf/postgres-operator/pgo.yaml The pgo.yaml file sets many different Operator configuration settings and is described in the pgo.yaml configuration documentation section.\nThe pgo.yaml file is deployed along with the other Operator configuration files when you run:\nmake deployoperator  conf/postgres-operator Directory Files within the conf/postgres-operator directory contain various templates that are used by the Operator when creating Kubernetes resources. In an advanced Operator deployment, administrators can modify these templates to add their own custom meta-data or make other changes to influence the Resources that get created on your Kubernetes cluster by the Operator.\nFiles within this directory are used specifically when creating PostgreSQL Cluster resources. Sidecar components such as pgBouncer and pgPool II templates are also located within this directory.\nAs with the other Operator templates, administrators can make custom changes to this set of templates to add custom features or metadata into the Resources created by the Operator.\nSecurity Setting up pgo users and general security configuration is described in the Security section of this documentation.\nLocal pgo CLI Configuration You can specify the default namespace you want to use by setting the PGO_NAMESPACE environment variable locally on the host the pgo CLI command is running.\nexport PGO_NAMESPACE=pgouser1  When that variable is set, each command you issue with pgo will use that namespace unless you over-ride it using the \u0026ndash;namespace command line flag.\npgo show cluster foo --namespace=pgouser2  Namespace Configuration The Design Design section of this documentation talks further about the use of namespaces within the Operator and configuring different deployment models of the Operator.\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/gettingstarted/",
	"title": "Getting Started",
	"tags": [],
	"description": "",
	"content": "If you are new to the Crunchy PostgreSQL Operator and interested in installing the Crunchy PostgreSQL Operator in your environment, please start here: - Installation via Bash - Installation via Ansible\nIf you have the Crunchy PostgreSQL Operator installed in your environment, and are interested in installation of the client interface, please start here: - PGO Client Install\nIf you have the Crunchy PostgreSQL and Client Interface installed in your environment and are interested in guidance on the use of the Crunchy PostgreSQL Operator, please start here: - PGO CLI Overview\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/gettingstarted/design/designoverview/",
	"title": "Design",
	"tags": [],
	"description": "",
	"content": " Provisioning So, what does the Postgres Operator actually deploy when you create a cluster?\nOn this diagram, objects with dashed lines are components that are optionally deployed as part of a PostgreSQL Cluster by the operator. Objects with solid lines are the fundamental and required components.\nFor example, within the Primary Deployment, the metrics container is completely optional. That component can be deployed using either the operator configuration or command line arguments if you want to cause metrics to be collected from the Postgres container.\nReplica deployments are similar to the primary deployment but are optional. A replica is not required to be created unless the capability for one is necessary. As you scale up the Postgres cluster, the standard set of components gets deployed and replication to the primary is started.\nNotice that each cluster deployment gets its own unique Persistent Volumes. Each volume can use different storage configurations which provides fined grained placement of the database data files.\nThere is a Service created for the primary Postgres deployment and a Service created for any replica Postgres deployments within a given Postgres cluster. Primary services match Postgres deployments using a label service-name of the following format:\nservice-name=mycluster service-name=mycluster-replica  Custom Resource Definitions Kubernetes Custom Resource Definitions are used in the design of the PostgreSQL Operator to define the following:\n Cluster - pgclusters Backup - pgbackups Policy - pgpolicies Tasks - pgtasks  Metadata about the Postgres cluster deployments are stored within these CRD resources which act as the source of truth for the Operator.\nThe postgres-operator design incorporates the following concepts:\nEvent Listeners Kubernetes events are created for the Operator CRD resources when new resources are made, deleted, or updated. These events are processed by the Operator to perform asynchronous actions.\nAs events are captured, controller logic is executed within the Operator to perform the bulk of operator logic.\nREST API A feature of the Operator is to provide a REST API upon which users or custom applications can inspect and cause actions within the Operator such as provisioning resources or viewing status of resources.\nThis API is secured by a RBAC (role based access control) security model whereby each API call has a permission assigned to it. API roles are defined to provide granular authorization to Operator services.\nCommand Line Interface One of the unique features of the Operator is the pgo command line interface (CLI). This tool is used by a normal end-user to create databases or clusters, or make changes to existing databases.\nThe CLI interacts with the REST API deployed within the postgres-operator deployment.\nNode Affinity You can have the Operator add a node affinity section to a new Cluster Deployment if you want to cause Kubernetes to attempt to schedule a primary cluster to a specific Kubernetes node.\nYou can see the nodes on your Kube cluster by running the following:\nkubectl get nodes  You can then specify one of those names (e.g. kubeadm-node2) when creating a cluster;\npgo create cluster thatcluster --node-label=kubeadm-node2  The affinity rule inserted in the Deployment uses a preferred strategy so that if the node were down or not available, Kubernetes will go ahead and schedule the Pod on another node.\nWhen you scale up a Cluster and add a replica, the scaling will take into account the use of --node-label. If it sees that a cluster was created with a specific node name, then the replica Deployment will add an affinity rule to attempt to schedule\nFail-over Manual and automated fail-over are supported in the Operator within a single Kubernetes cluster.\nManual failover is performed by API actions involving a query and then a target being specified to pick the fail-over replica target.\nAutomatic fail-over is performed by the Operator by evaluating the readiness of a primary. Automated fail-over can be globally specified for all clusters or specific clusters.\nUsers can configure the Operator to replace a failed primary with a new replica if they want that behavior.\nThe fail-over logic includes:\n deletion of the failed primary Deployment pick the best replica to become the new primary label change of the targeted Replica to match the primary Service execute the PostgreSQL promote command on the targeted replica  pgbackrest Integration The Operator integrates various features of the pgbackrest backup and restore project. A key component added to the Operator is the pgo-backrest-repo container, this container acts as a pgBackRest remote repository for the Postgres cluster to use for storing archive files and backups.\nThe following diagrams depicts some of the integration features:\nIn this diagram, starting from left to right we see the following:\n a user when they enter pgo backup mycluster \u0026ndash;backup-type=pgbackrest will cause a pgo-backrest container to be run as a Job, that container will execute a pgbackrest backup command in the pgBackRest repository container to perform the backup function.\n a user when they enter pgo show backup mycluster \u0026ndash;backup-type=pgbackrest will cause a pgbackrest info command to be executed on the pgBackRest repository container, the info output is sent directly back to the user to view\n the Postgres container itself will use an archive command, pgbackrest archive-push to send archives to the pgBackRest repository container\n the user entering pgo create cluster mycluster \u0026ndash;pgbackrest will cause a pgBackRest repository container deployment to be created, that repository is exclusively used for this Postgres cluster\n lastly, a user entering pgo restore mycluster will cause a pgo-backrest-restore container to be created as a Job, that container executes the pgbackrest restore command\n  pgbackrest Restore The pgbackrest restore command is implemented as the pgo restore command. This command is destructive in the sense that it is meant to restore a PG cluster meaning it will revert the PG cluster to a restore point that is kept in the pgbackrest repository. The prior primary data is not deleted but left in a PVC to be manually cleaned up by a DBA. The restored PG cluster will work against a new PVC created from the restore workflow.\nWhen doing a pgo restore, here is the workflow the Operator executes:\n turn off autofail if it is enabled for this PG cluster allocate a new PVC to hold the restored PG data delete the the current primary database deployment update the pgbackrest repo for this PG cluster with a new data path of the new PVC create a pgo-backrest-restore job, this job executes the pgbackrest restore command from the pgo-backrest-restore container, this Job mounts the newly created PVC once the restore job completes, a new primary Deployment is created which mounts the restored PVC volume  At this point the PG database is back in a working state. DBAs are still responsible to re-enable autofail using pgo update cluster and also perform a pgBackRest backup after the new primary is ready. This version of the Operator also does not handle any errors in the PG replicas after a restore, that is left for the DBA to handle.\nOther things to take into account before you do a restore:\n if a schedule has been created for this PG cluster, delete that schedule prior to performing a restore If your database has been paused after the target restore was completed, then you would need to run the psql command select pg_wal_replay_resume() to complete the recovery, on PG 9.6\u0026frasl;9.5 systems, the command you will use is select pg_xlog_replay_resume(). You can confirm the status of your database by using the built in postgres admin functions found here: a pgBackRest restore is destructive in the sense that it deletes the existing primary deployment for the cluster prior to creating a new deployment containing the restored primary database. However, in the event that the pgBackRest restore job fails, the pgo restore command be can be run again, and instead of first deleting the primary deployment (since one no longer exists), a new primary will simply be created according to any options specified. Additionally, even though the original primary deployment will be deleted, the original primary PVC will remain. there is currently no Operator validation of user entered pgBackRest command options, you will need to make sure to enter these correctly, if not the pgBackRest restore command can fail. the restore workflow does not perform a backup after the restore nor does it verify that any replicas are in a working status after the restore, it is possible you might have to take actions on the replica to get them back to replicating with the new restored primary. pgbackrest.org suggests running a pgbackrest backup after a restore, this needs to be done by the DBA as part of a restore when performing a pgBackRest restore, the node-label flag can be utilized to target a specific node for both the pgBackRest restore job and the new (i.e. restored) primary deployment that is then created for the cluster. If a node label is not specified, the restore job will not target any specific node, and the restored primary deployment will inherit any node labels defined for the original primary deployment.  pgbackrest AWS S3 Support The Operator supports the use AWS S3 storage buckets for the pgbackrest repository in any pgbackrest-enabled cluster. When S3 support is enabled for a cluster, all archives will automatically be pushed to a pre-configured S3 storage bucket, and that same bucket can then be utilized for the creation of any backups as well as when performing restores. Please note that once a storage type has been selected for a cluster during cluster creation (specifically local, s3, or both, as described in detail below), it cannot be changed.\nThe Operator allows for the configuration of a single storage bucket, which can then be utilized across multiple clusters. Once S3 support has been enabled for a cluster, pgbackrest will create a backrestrepo directory in the root of the configured S3 storage bucket (if it does not already exist), and subdirectories will then be created under the backrestrepo directory for each cluster created with S3 storage enabled.\nS3 Configuration In order to enable S3 storage, you must provide the required AWS S3 configuration information prior to deploying the Operator. First, you will need to add the proper S3 bucket name, AWS S3 endpoint and AWS S3 region to the Cluster section of the pgo.yaml configuration file (additional information regarding the configuration of the pgo.yaml file can be found here) :\nCluster: BackrestS3Bucket: containers-dev-pgbackrest BackrestS3Endpoint: s3.amazonaws.com BackrestS3Region: us-east-1 You will then need to specify the proper credentials for authenticating into the S3 bucket specified by adding a key and key secret to the $PGOROOT/pgo-backrest-repo/aws-s3-credentials.yaml configuration file:\n--- aws-s3-key: ABCDEFGHIJKLMNOPQRST aws-s3-key-secret: ABCDEFG/HIJKLMNOPQSTU/VWXYZABCDEFGHIJKLM Once the above configuration details have been provided, you can deploy the Operator per the PGO installation instructions.\nEnabling S3 Storage in a Cluster With S3 storage properly configured within your PGO installation, you can now select either local storage, S3 storage, or both when creating a new cluster. The type of storage selected upon creation of the cluster will determine the type of storage that can subsequently be used when performing pgbackrest backups and restores. A storage type is specified using the --pgbackrest-storage-type flag, and can be one of the following values:\n local - pgbackrest will use volumes local to the container (e.g. Persistent Volumes) for storing archives, creating backups and locating backups for restores. This is the default value for the --pgbackrest-storage-type flag. s3 - pgbackrest will use the pre-configured AWS S3 storage bucket for storing archives, creating backups and locating backups for restores local,s3 (both) - pgbackrest will use both volumes local to the container (e.g. persistent volumes), as well as the pre-configured AWS S3 storage bucket, for storing archives. Also allows the use of local and/or S3 storage when performing backups and restores.  For instance, the following command enables both local and s3 storage in a new cluster:\npgo create cluster mycluster --pgbackrest --pgbackrest-storage-type=local,s3 -n pgouser1 As described above, this will result in pgbackrest pushing archives to both local and S3 storage, while also allowing both local and S3 storage to be utilized for backups and restores. However, you could also enable S3 storage only when creating the cluster:\npgo create cluster mycluster --pgbackrest --pgbackrest-storage-type=s3 -n pgouser1 Now all archives for the cluster will be pushed to S3 storage only, and local storage will not be utilized for storing archives (nor can local storage be utilized for backups and restores).\nUsing S3 to Backup \u0026amp; Restore As described above, once S3 storage has been enabled for a cluster, it can also be used when backing up or restoring a cluster. Here a both local and S3 storage is selected when performing a backup:\npgo backup mycluster --backup-type=pgbackrest --pgbackrest-storage-type=local,s3 -n pgouser1 This results in pgbackrest creating a backup in a local volume (e.g. a persistent volume), while also creating an additional backup in the configured S3 storage bucket. However, a backup can be created using S3 storage only:\npgo backup mycluster --backup-type=pgbackrest --pgbackrest-storage-type=s3 -n pgouser1 Now pgbackrest will only create a backup in the S3 storage bucket only.\nWhen performing a restore, either local or s3 must be selected (selecting both for a restore will result in an error). For instance, the following command specifies S3 storage for the restore:\npgo restore mycluster --pgbackrest-storage-type=s3 -n pgouser1 This will result in a full restore utilizing the backups and archives stored in the configured S3 storage bucket.\nPlease note that because local is the default storage type for the backup and restore commands, s3 must be explicitly set using the --pgbackrest-storage-type flag when performing backups and restores on clusters where only S3 storage is enabled.\nAWS Certificate Authority The Operator installation includes a default certificate bundle that is utilized by default to establish trust between pgbackrest and the AWS S3 endpoint used for S3 storage. Please modify or replace this certificate bundle as needed prior to deploying the Operator if another certificate authority is needed to properly establish trust between pgbackrest and your S3 endpoint.\nThe certificate bundle can be found here: $PGOROOT/pgo-backrest-repo/aws-s3-ca.crt.\nWhen modifying or replacing the certificate bundle, please be sure to maintain the same path and filename.\nPGO Scheduler The Operator includes a cronlike scheduler application called pgo-scheduler. Its purpose is to run automated tasks such as PostgreSQL backups or SQL policies against PostgreSQL clusters created by the Operator.\nPGO Scheduler watches Kubernetes for configmaps with the label crunchy-scheduler=true in the same namespace the Operator is deployed. The configmaps are json objects that describe the schedule such as:\n Cron like schedule such as: * * * * * Type of task: pgbackrest, pgbasebackup or policy  Schedules are removed automatically when the configmaps are deleted.\nPGO Scheduler uses the UTC timezone for all schedules.\nSchedule Expression Format Schedules are expressed using the following rules:\nField name | Mandatory? | Allowed values | Allowed special characters ---------- | ---------- | -------------- | -------------------------- Seconds | Yes | 0-59 | * / , - Minutes | Yes | 0-59 | * / , - Hours | Yes | 0-23 | * / , - Day of month | Yes | 1-31 | * / , - ? Month | Yes | 1-12 or JAN-DEC | * / , - Day of week | Yes | 0-6 or SUN-SAT | * / , - ?  pgBackRest Schedules pgBackRest schedules require pgBackRest enabled on the cluster to backup. The scheduler will not do this on its own.\npgBaseBackup Schedules pgBaseBackup schedules require a backup PVC to already be created. The operator will make this PVC using the backup commands:\npgo backup mycluster  Policy Schedules Policy schedules require a SQL policy already created using the Operator. Additionally users can supply both the database in which the policy should run and a secret that contains the username and password of the PostgreSQL role that will run the SQL. If no user is specified the scheduler will default to the replication user provided during cluster creation.\nCustom Resource Definitions The Operator makes use of custom resource definitions to maintain state and resource definitions as offered by the Operator.\nIn this above diagram, the CRDs heavily used by the Operator include:\n pgcluster - defines the Postgres cluster definition, new cluster requests are captured in a unique pgcluster resource for that Postgres cluster pgtask - workflow and other related administration tasks are captured within a set of pgtasks for a given pgcluster pgbackup - when you run a pgbasebackup, a pgbackup is created to hold the workflow and status of the last backup job, this CRD will eventually be deprecated in favor of a more general pgtask resource pgreplica - when you create a Postgres replica, a pgreplica CRD is created to define that replica  Considerations for Multi-zone Cloud Environments Overview When using the Operator in a Kubernetes cluster consisting of nodes that span multiple zones, special consideration must be taken to ensure all pods and the volumes they require are scheduled and provisioned within the same zone. Specifically, being that a pod is unable mount a volume that is located in another zone, any volumes that are dynamically provisioned must be provisioned in a topology-aware manner according to the specific scheduling requirements for the pod. For instance, this means ensuring that the volume containing the database files for the primary DB in a new PG cluster is provisioned in the same zone as the node containing the PG primary pod that will be using it.\nDefault Behavior By default, the Kubernetes scheduler will ensure any pods created that claim a specific volume via a PVC are scheduled on a node in the same zone as that volume. This is part of the multi-zone support that is included in Kubernetes by default. However, when using dynamic provisioning, volumes are not provisioned in a topology-aware manner by default, which means a volume will not be provisioned according to the same scheduling requirements that will be placed on the pod that will be using it (e.g. it will not consider node selectors, resource requirements, pod affinity/anti-affinity, and various other scheduling requirements). Rather, PVCs are immediately bound as soon as they are requested, which means volumes are provisioned without knowledge of these scheduling requirements. This behavior is the result of the volumeBindingMode defined on the Storage Class being utilized to dynamically provision the volume, which is set to Immediate by default. This can be seen in the following Storage Class definition, which defines a Storage Class for a Google Cloud Engine Persistent Disk (GCE PD) that uses the default value of Immediate for its volumeBindingMode:\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: example-sc provisioner: kubernetes.io/gce-pd parameters: type: pd-standard volumeBindingMode: Immediate Unfortunately, using Immediate for the volumeBindingMode in a multi-zone cluster can result in undesired behavior when using the Operator, being that the scheduler will ignore any requested (but not mandatory) scheduling requirements if necessary to ensure the pod can be scheduled. Specifically, the scheduler will ultimately schedule the pod on a node in the same zone as the volume, even if another node was requested for scheduling that pod. For instance, a node label might be specified using the --node-label option when creating a cluster using the pgo create cluster command in order target a specific node (or nodes) for the deployment of that cluster. Within the Operator, a node label is implemented as a preferredDuringSchedulingIgnoredDuringExecution node affinity rule, which is an affinity rule that Kubernetes will attempt to adhere to when scheduling any pods for the cluster, but will not guarantee (more information on node affinity rules can be found here). Therefore, if the volume ends up in a zone other than the zone containing the node (or nodes) defined by the node label, the node label will be ignored, and the pod will be scheduled according to the zone containing the volume.\nTopology Aware Volumes In order to overcome the behavior described above in a multi-zone cluster, volumes must be made topology aware. This is accomplished by setting the volumeBindingMode for the storage class to WaitForFirstConsumer, which delays the dynamic provisioning of a volume until a pod using it is created. In other words, the PVC is no longer bound as soon as it is requested, but rather waits for a pod utilizing it to be creating prior to binding. This change ensures that volume can take into account the scheduling requirements for the pod, which in the case of a multi-zone cluster means ensuring the volume is provisioned in the same zone containing the node where the pod has be scheduled. This also means the scheduler should no longer ignore a node label in order to follow a volume to another zone when scheduling a pod, since the volume will now follow the pod according to the pods specificscheduling requirements. The following is an example of the the same Storage Class defined above, only with volumeBindingMode now set to WaitForFirstConsumer:\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: example-sc provisioner: kubernetes.io/gce-pd parameters: type: pd-standard volumeBindingMode: WaitForFirstConsumer Additional Solutions If you are using a version of Kubernetes that does not support WaitForFirstConsumer, an alternate (and now deprecated) solution exists in the form of parameters that can be defined on the Storage Class definition to ensure volumes are provisioned in a specific zone (or zones). For instance, when defining a Storage Class for a GCE PD for use in Google Kubernetes Engine (GKE) cluster, the zone parameter can be used to ensure any volumes dynamically provisioned using that Storage Class are located in that specific zone. The following is an example of a Storage Class for a GKE cluster that will provision volumes in the us-east1 zone:\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata: name: example-sc provisioner: kubernetes.io/gce-pd parameters: type: pd-standard replication-type: none zone: us-east1 Once storage classes have been defined for one or more zones, they can then be defined as one or more storage configurations within the pgo.yaml configuration file (as described in the PGO YAML configuration guide). From there those storage configurations can then be selected when creating a new cluster, as shown in the following example:\npgo create cluster mycluster --storage-config=example-sc With this approach, the pod will once again be scheduled according to the zone in which the volume was provisioned. However, the zone parameters defined on the Storage Class bring consistency to scheduling by guaranteeing that the volume, and therefore also the pod using that volume, are scheduled in a specific zone as defined by the user, bringing consistency and predictability to volume provisioning and pod scheduling in multi-zone clusters.\nFor more information regarding the specific parameters available for the Storage Classes being utilizing in your cloud environment, please see the Kubernetes documentation for Storage Classes.\nLastly, while the above applies to the dynamic provisioning of volumes, it should be noted that volumes can also be manually provisioned in desired zones in order to achieve the desired topology requirements for any pods and their volumes.\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/",
	"title": "Installation",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/configuration/pgo-yaml-configuration/",
	"title": "PGO YAML",
	"tags": [],
	"description": "",
	"content": " pgo.yaml Configuration The pgo.yaml file contains many different configuration settings as described in this section of the documentation.\nThe pgo.yaml file is broken into major sections as described below:\nCluster    Setting Definition     BasicAuth if set to true will enable Basic Authentication   PrimaryNodeLabel newly created primary deployments will specify this node label if specified, unless you override it using the \u0026ndash;node-label command line flag, if not set, no node label is specifed   ReplicaNodeLabel newly created replica deployments will specify this node label if specified, unless you override it using the \u0026ndash;node-label command line flag, if not set, no node label is specifed   CCPImagePrefix newly created containers will be based on this image prefix (e.g. crunchydata), update this if you require a custom image prefix   CCPImageTag newly created containers will be based on this image version (e.g. centos7-11.3-2.4.0), unless you override it using the \u0026ndash;ccp-image-tag command line flag   Port the PostgreSQL port to use for new containers (e.g. 5432)   LogStatement postgresql.conf log_statement value (required field)   LogMinDurationStatement postgresql.conf log_min_duration_statement value (required field)   User the PostgreSQL normal user name   Database the PostgreSQL normal user database   Replicas the number of cluster replicas to create for newly created clusters, typically users will scale up replicas on the pgo CLI command line but this global value can be set as well   PgmonitorPassword the password to use for pgmonitor metrics collection if you specify \u0026ndash;metrics when creating a PG cluster   Metrics boolean, if set to true will cause each new cluster to include crunchy-collect as a sidecar container for metrics collection, if set to false (default), users can still add metrics on a cluster-by-cluster basis using the pgo command flag \u0026ndash;metrics   Badger boolean, if set to true will cause each new cluster to include crunchy-pgbadger as a sidecar container for static log analysis, if set to false (default), users can still add pgbadger on a cluster-by-cluster basis using the pgo create cluster command flag \u0026ndash;pgbadger   Policies optional, list of policies to apply to a newly created cluster, comma separated, must be valid policies in the catalog   PasswordAgeDays optional, if set, will set the VALID UNTIL date on passwords to this many days in the future when creating users or setting passwords, defaults to 60 days   PasswordLength optional, if set, will determine the password length used when creating passwords, defaults to 8   ServiceType optional, if set, will determine the service type used when creating primary or replica services, defaults to ClusterIP if not set, can be overridden by the user on the command line as well   Backrest optional, if set, will cause clusters to have the pgbackrest volume PVC provisioned during cluster creation   BackrestPort currently required to be port 2022   Autofail optional, if set, will cause clusters to be checked for auto failover in the event of a non-Ready status   AutofailReplaceReplica optional, default is false, if set, will determine whether a replica is created as part of a failover to replace the promoted replica, the AutofailReplaceReplica setting in pgo.yaml is overrode with this command line flag if specified by a user.    Storage    Setting Definition     PrimaryStorage required, the value of the storage configuration to use for the primary PostgreSQL deployment   BackupStorage required, the value of the storage configuration to use for backups, including the storage for pgbackrest repo volumes   ReplicaStorage required, the value of the storage configuration to use for the replica PostgreSQL deployments   ReplicaStorage required, the value of the storage configuration to use for the replica PostgreSQL deployments   BackrestStorage required, the value of the storage configuration to use for the pgbackrest shared repository deployment created when a user specifies pgbackrest to be enabled on a cluster   StorageClass for a dynamic storage type, you can specify the storage class used for storage provisioning(e.g. standard, gold, fast)   AccessMode the access mode for new PVCs (e.g. ReadWriteMany, ReadWriteOnce, ReadOnlyMany). See below for descriptions of these.   Size the size to use when creating new PVCs (e.g. 100M, 1Gi)   Storage.storage1.StorageType supported values are either dynamic, create, if not supplied, create is used   Fsgroup optional, if set, will cause a SecurityContext and fsGroup attributes to be added to generated Pod and Deployment definitions   SupplementalGroups optional, if set, will cause a SecurityContext to be added to generated Pod and Deployment definitions   MatchLabels optional, if set, will cause the PVC to add a matchlabels selector in order to match a PV, only useful when the StorageType is create, when specified a label of key=value is added to the PVC as a match criteria    Storage Configuration Examples In pgo.yaml, you will need to configure your storage configurations depending on which storage you are wanting to use for Operator provisioning of Persistent Volume Claims. The examples below are provided as a sample. In all the examples you are free to change the Size to meet your requirements of Persistent Volume Claim size.\nHostPath Example HostPath is provided for simple testing and use cases where you only intend to run on a single Linux host for your Kubernetes cluster.\n hostpathstorage: AccessMode: ReadWriteMany Size: 1G StorageType: create  NFS Example In the following NFS example, notice that the SupplementalGroups setting is set, this can be whatever GID you have your NFS mount set to, typically we set this nfsnobody as below. NFS file systems offer a ReadWriteMany access mode.\n nfsstorage: AccessMode: ReadWriteMany Size: 1G StorageType: create SupplementalGroups: 65534  Storage Class Example In the following example, the important attribute to set for a typical Storage Class is the Fsgroup setting. This value is almost always set to 26 which represents the Postgres user ID that the Crunchy Postgres container runs as. Most Storage Class providers offer ReadWriteOnce access modes, but refer to your provider documentation for other access modes it might support.\n storageos: AccessMode: ReadWriteOnce Size: 1G StorageType: dynamic StorageClass: fast Fsgroup: 26  Container Resources    Setting Definition     DefaultContainerResource optional, the value of the container resources configuration to use for all database containers, if not set, no resource limits or requests are added on the database container   DefaultLoadResource optional, the value of the container resources configuration to use for pgo-load containers, if not set, no resource limits or requests are added on the database container   DefaultLspvcResource optional, the value of the container resources configuration to use for pgo-lspvc containers, if not set, no resource limits or requests are added on the database container   DefaultRmdataResource optional, the value of the container resources configuration to use for pgo-rmdata containers, if not set, no resource limits or requests are added on the database container   DefaultBackupResource optional, the value of the container resources configuration to use for crunchy-backup containers, if not set, no resource limits or requests are added on the database container   DefaultPgbouncerResource optional, the value of the container resources configuration to use for crunchy-pgbouncer containers, if not set, no resource limits or requests are added on the database container   DefaultPgpoolResource optional, the value of the container resources configuration to use for crunchy-pgpool containers, if not set, no resource limits or requests are added on the database container   RequestsMemory request size of memory in bytes   RequestsCPU request size of CPU cores   LimitsMemory request size of memory in bytes   LimitsCPU request size of CPU cores    Miscellaneous (Pgo)    Setting Definition     PreferredFailoverNode optional, a label selector (e.g. hosttype=offsite) that if set, will be used to pick the failover target which is running on a host that matches this label if multiple targets are equal in replication status   COImagePrefix image tag prefix to use for the Operator containers   COImageTag image tag to use for the Operator containers   Audit boolean, if set to true will cause each apiserver call to be logged with an audit marking    Storage Configuration Details You can define n-number of Storage configurations within the pgo.yaml file. Those Storage configurations follow these conventions -\n they must have lowercase name (e.g. storage1) they must be unique names (e.g. mydrstorage, faststorage, slowstorage)  These Storage configurations are referenced in the BackupStorage, ReplicaStorage, and PrimaryStorage configuration values. However, there are command line options in the pgo client that will let a user override these default global values to offer you the user a way to specify very targeted storage configurations when needed (e.g. disaster recovery storage for certain backups).\nYou can set the storage AccessMode values to the following:\n ReadWriteMany - mounts the volume as read-write by many nodes ReadWriteOnce - mounts the PVC as read-write by a single node ReadOnlyMany - mounts the PVC as read-only by many nodes  These Storage configurations are validated when the pgo-apiserver starts, if a non-valid configuration is found, the apiserver will abort. These Storage values are only read at apiserver start time.\nThe following StorageType values are possible -\n dynamic - this will allow for dynamic provisioning of storage using a StorageClass. create - This setting allows for the creation of a new PVC for each PostgreSQL cluster using a naming convention of clustername. When set, the Size, AccessMode settings are used in constructing the new PVC.  The operator will create new PVCs using this naming convention: dbname where dbname is the database name you have specified. For example, if you run:\npgo create cluster example1 -n pgouser1  It will result in a PVC being created named example1 and in the case of a backup job, the pvc is named example1-backup\nNote, when Storage Type is create, you can specify a storage configuration setting of MatchLabels, when set, this will cause a selector of key=value to be added into the PVC, this will let you target specific PV(s) to be matched for this cluster. Note, if a PV does not match the claim request, then the cluster will not start. Users that want to use this feature have to place labels on their PV resources as part of PG cluster creation before creating the PG cluster. For example, users would add a label like this to their PV before they create the PG cluster:\nkubectl label pv somepv myzone=somezone -n pgouser1  If you do not specify MatchLabels in the storage configuration, then no match filter is added and any available PV will be used to satisfy the PVC request. This option does not apply to dynamic storage types.\nExample PV creation scripts are provided that add labels to a set of PVs and can be used for testing: $COROOT/pv/create-pv-nfs-labels.sh in that example, a label of crunchyzone=red is set on a set of PVs to test with.\nThe pgo.yaml includes a storage config named nfsstoragered that when used will demonstrate the label matching. This feature allows you to support n-number of NFS storage configurations and supports spreading a PG cluster across different NFS storage configurations.\nContainer Resources Details In the pgo.yaml configuration file you have the option to configure a default container resources configuration that when set will add CPU and memory resource limits and requests values into each database container when the container is created.\nYou can also override the default value using the --resources-config command flag when creating a new cluster:\npgo create cluster testcluster --resources-config=large -n pgouser1  Note, if you try to allocate more resources than your host or Kube cluster has available then you will see your pods wait in a Pending status. The output from a kubectl describe pod command will show output like this in this event: Events:\n Type Reason Age From Message ---- ------ ---- ---- ------- Warning FailedScheduling 49s (x8 over 1m) default-scheduler No nodes are available that match all of the predicates: Insufficient memory (1).  Overriding Storage Configuration Defaults pgo create cluster testcluster --storage-config=bigdisk -n pgouser1  That example will create a cluster and specify a storage configuration of bigdisk to be used for the primary database storage. The replica storage will default to the value of ReplicaStorage as specified in pgo.yaml.\npgo create cluster testcluster2 --storage-config=fastdisk --replica-storage-config=slowdisk -n pgouser1  That example will create a cluster and specify a storage configuration of fastdisk to be used for the primary database storage, while the replica storage will use the storage configuration slowdisk.\npgo backup testcluster --storage-config=offsitestorage -n pgouser1  That example will create a backup and use the offsitestorage storage configuration for persisting the backup.\nUsing Storage Configurations for Disaster Recovery A simple mechanism for partial disaster recovery can be obtained by leveraging network storage, Kubernetes storage classes, and the storage configuration options within the Operator.\nFor example, if you define a Kubernetes storage class that refers to a storage backend that is running within your disaster recovery site, and then use that storage class as a storage configuration for your backups, you essentially have moved your backup files automatically to your disaster recovery site thanks to network storage.\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/configuration/",
	"title": "Configuration",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/gettingstarted/design/custom-config/",
	"title": "Custom Configuration",
	"tags": [],
	"description": "",
	"content": " Custom Postgres Configurations Users and administrators can specify a custom set of Postgres configuration files be used when creating a new Postgres cluster. The configuration files you can change include -\n postgresql.conf pg_hba.conf setup.sql  Different configurations for PostgreSQL might be defined for the following -\n OLTP types of databases OLAP types of databases High Memory Minimal Configuration for Development Project Specific configurations Special Security Requirements  Global ConfigMap If you create a configMap called pgo-custom-pg-config with any of the above files within it, new clusters will use those configuration files when setting up a new database instance. You do NOT have to specify all of the configuration files. It is entirely up to your use case to determine which to use.\nAn example set of configuration files and a script to create the global configMap is found at\n$PGOROOT/examples/custom-config  If you run the create.sh script there, it will create the configMap that will include the PostgreSQL configuration files within that directory.\nConfig Files Purpose The postgresql.conf file is the main Postgresql configuration file that allows the definition of a wide variety of tuning parameters and features.\nThe pg_hba.conf file is the way Postgresql secures client access.\nThe setup.sql file is a Crunchy Container Suite configuration file used to initially populate the database after the initial initdb is run when the database is first created. Changes would be made to this if you wanted to define which database objects are created by default.\nGranular Config Maps Granular config maps can be defined if it is necessary to use a different set of configuration files for different clusters rather than having a single configuration (e.g. Global Config Map). A specific set of ConfigMaps with their own set of PostgreSQL configuration files can be created. When creating new clusters, a --custom-config flag can be passed along with the name of the ConfigMap which will be used for that specific cluster or set of clusters.\nDefaults If there is no reason to change the default PostgreSQL configuration files that ship with the Crunchy Postgres container, there is no requirement to. In this event, continue using the Operator as usual and avoid defining a global configMap.\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/gettingstarted/design/",
	"title": "Design",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/gettingstarted/design/custom-config-ssl/",
	"title": "Custom SSL Configuration",
	"tags": [],
	"description": "",
	"content": " Custom Postgres SSL Configurations The Crunchy Data Postgres Operator can create clusters that use SSL authentication by utilizing custom configmaps.\nConfiguration Files for SSL Authentication Users and administrators can specify a custom set of Postgres configuration files to be used when creating a new Postgres cluster. This example uses the files below-\n postgresql.conf pg_hba.conf pg_ident.conf  along with generated security certificates, to setup a custom SSL configuration.\nConfig Files Purpose The postgresql.conf file is the main Postgresql configuration file that allows the definition of a wide variety of tuning parameters and features.\nThe pg_hba.conf file is the way Postgresql secures client access.\nThe pg_ident.conf is the ident map file and defines user name maps.\nConfigMap Creation This example shows how you can configure PostgreSQL to use SSL for client authentication.\nThe example requires SSL certificates and keys to be created. Included in the examples directory is the script called by create.sh to create self-signed certificates (server and client) for the example:\n$PGOROOT/examples/ssl-creator.sh.  Additionally, this script requires the certstrap utility to be installed. An install script is provided to install the correct version for the example if not already installed.\nThe relevant configuration files are located in the configs directory and will configure the clsuter to use SSL client authentication. These, along with the client certificate for the user \u0026lsquo;testuser\u0026rsquo; and a server certificate for \u0026lsquo;pgo-custom-ssl-container\u0026rsquo;, will make up the necessary configuration items to be stored in the \u0026lsquo;pgo-custom-ssl-config\u0026rsquo; configmap.\nExample Steps Run the script as follow:\ncd $PGOROOT/examples/custom-config-ssl ./create.sh  This will generate a configmap named \u0026lsquo;pgo-custom-ssl-config\u0026rsquo;.\nOnce this configmap is created, run\npgo create cluster customsslcluster --custom-config pgo-custom-ssl-config -n ${PGO_NAMESPACE}  A required step to make this example work is to define in your /etc/hosts file an entry that maps \u0026lsquo;pgo-custom-ssl-container\u0026rsquo; to the service cluster IP address for the container created above.\nFor instance, if your service has an address as follows:\n${PGO_CMD} get service -n ${PGO_NAMESPACE} NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE customsslcluster 172.30.211.108 \u0026lt;none\u0026gt; 5432/TCP  Then your /etc/hosts file needs an entry like this:\n172.30.211.108 pgo-custom-ssl-container  For production Kubernetes and OpenShift installations, it will likely be preferred for DNS names to resolve to the PostgreSQL service name and generate server certificates using the DNS names instead of the example name pgo-custom-ssl-container.\nIf as a client its required to confirm the identity of the server, verify-full can be specified for ssl-mode in the connection string. This will check if the server and the server certificate have the same name. Additionally, the proper connection parameters must be specified in the connection string for the certificate information required to trust and verify the identity of the server (sslrootcert and sslcrl), and to authenticate the client using a certificate (sslcert and sslkey):\npsql \u0026quot;postgresql://testuser@pgo-custom-ssl-container:5432/userdb?sslmode=verify-full\u0026amp;sslrootcert=/home/pgo/odev/src/github.com/crunchydata/postgres-operator/examples/custom-config-ssl/certs/ca.crt\u0026amp;sslcrl=/home/pgo/odev/src/github.com/crunchydata/postgres-operator/examples/custom-config-ssl/certs/ca.crl\u0026amp;sslcert=/home/pgo/odev/src/github.com/crunchydata/postgres-operator/examples/custom-config-ssl/certs/client.crt\u0026amp;sslkey=/home/pgo/odev/src/github.com/crunchydata/postgres-operator/examples/custom-config-ssl/certs/client.key\u0026quot;  To connect via IP, sslmode can be changed to require. This will verify the server by checking the certificate chain up to the trusted certificate authority, but will not verify that the hostname matches the certificate, as occurs with verify-full. The same connection parameters as above can be then provided for the client and server certificate information. i\npsql \u0026quot;postgresql://testuser@IP_OF_PGSQL:5432/userdb?sslmode=require\u0026amp;sslrootcert=/home/pgo/odev/src/github.com/crunchydata/postgres-operator/examples/custom-config-ssl/certs/ca.crt\u0026amp;sslcrl=/home/pgo/odev/src/github.com/crunchydata/postgres-operator/examples/custom-config-ssl/certs/ca.crl\u0026amp;sslcert=/home/pgo/odev/src/github.com/crunchydata/postgres-operator/examples/custom-config-ssl/certs/client.crt\u0026amp;sslkey=/home/pgo/odev/src/github.com/crunchydata/postgres-operator/examples/custom-config-ssl/certs/client.key\u0026quot;  You should see a connection that looks like the following:\npsql (11.3) SSL connection (protocol: TLSv1.2, cipher: ECDHE-RSA-AES256-GCM-SHA384, bits: 256, compression: off) Type \u0026quot;help\u0026quot; for help. userdb=\u0026gt;  Important Notes Because SSL will be required for connections, certain features of the Operator will not function as expected. These include the following:\npgo test pgo load pgo apply  "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/gettingstarted/design/namespace/",
	"title": "Namespace",
	"tags": [],
	"description": "",
	"content": " Operator Namespaces The Operator itself knows which namespace it is running within by referencing the PGO_OPERATOR_NAMESPACE environment variable at startup time from within its Deployment definition.\nThe PGO_OPERATOR_NAMESPACE environment variable a user sets in their .bashrc file is used to determine what namespace the Operator is deployed into. The PGO_OPERATOR_NAMESPACE variable is referenced by the Operator during deployment.\nThe .bashrc NAMESPACE environment variable a user sets determines which namespaces the Operator will watch.\nNamespace Watching The Operator at startup time determines which namespaces it will service based on what it finds in the NAMESPACE environment variable that is passed into the Operator containers within the deployment.json file.\nThe NAMESPACE variable can hold different values which determine the namespaces which will be watched by the Operator.\nThe format of the NAMESPACE value is modeled after the following document:\nhttps://github.com/operator-framework/operator-lifecycle-manager/blob/master/Documentation/design/operatorgroups.md\nOwnNamespace Example Prior to version 4.0, the Operator was deployed into a single namespace and Postgres Clusters created by it were created in that same namespace.\nTo achieve that same deployment model you would use variable settings as follows:\nexport PGO_OPERATOR_NAMESPACE=pgo export NAMESPACE=pgo  SingleNamespace Example To have the Operator deployed into its own namespace but create Postgres Clusters into a different namespace the variables would be as follows:\nexport PGO_OPERATOR_NAMESPACE=pgo export NAMESPACE=pgouser1  MultiNamespace Example To have the Operator deployed into its own namespace but create Postgres Clusters into more than one namespace the variables would be as follows:\nexport PGO_OPERATOR_NAMESPACE=pgo export NAMESPACE=pgouser1,pgouser2  AllNamespaces Example To have the Operator deployed into its own namespace but create Postgres Clusters into any target namespace the variables would be as follows:\nexport PGO_OPERATOR_NAMESPACE=pgo export NAMESPACE=  Here the empty string value represents all namespaces.\nRBAC To support multiple namespace watching, the Operator deployment process changes somewhat from 3.X releases.\nEach namespace to be watched requires its own copy of the the following resources for working with the Operator:\nserviceaccount/pgo-backrest secret/pgo-backrest-repo-config role/pgo-role rolebinding/pgo-role-binding role/pgo-backrest-role rolebinding/pgo-backrest-role-binding  When you run the install-rbac.sh script, it iterates through the list of namespaces to be watched and creates these resources into each of those namespaces.\nIf you need to add a new namespace that the Operator will watch after an initial execution of install-rbac.sh, you will need to run the following for each new namespace:\ncreate-target-rbac.sh YOURNEWNAMESPACE $PGO_OPERATOR_NAMESPACE  The example deployment creates the following RBAC structure on your Kube system after running the install scripts:\npgo Clients and Namespaces The pgo CLI now is required to identify which namespace it wants to use when issuing commands to the Operator.\nUsers of pgo can either create a PGO_NAMESPACE environment variable to set the namespace in a persistent manner or they can specify it on the pgo command line using the \u0026ndash;namespace flag.\nIf a pgo request doe not contain a valid namespace the request will be rejected.\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/pgo-overview/",
	"title": "Operator CLI Overview",
	"tags": [],
	"description": "",
	"content": " The command line tool, pgo, is used to interact with the Postgres Operator.\nMost users will work with the Operator using the pgo CLI tool. That tool is downloaded from the GitHub Releases page for the Operator (https://github.com/crunchydata/postgres-operator/releases).\nThe pgo client is provided in Mac, Windows, and Linux binary formats, download the appropriate client to your local laptop or workstation to work with a remote Operator.\nSyntax Use the following syntax to run pgo commands from your terminal window:\npgo [command] ([TYPE] [NAME]) [flags]  Where command is a verb like:\n show get create delete  And type is a resource type like:\n cluster policy user  And name is the name of the resource type like:\n mycluster somesqlpolicy john  To get detailed help information and command flag descriptions on each pgo command, enter:\npgo [command] -h  Operations The following table shows the pgo operations currently implemented:\n   Operation Syntax Description     apply pgo apply mypolicy \u0026ndash;selector=name=mycluster Apply a SQL policy on a Postgres cluster(s) that have a label matching service-name=mycluster   backup pgo backup mycluster Perform a backup on a Postgres cluster(s)   create pgo create cluster mycluster Create an Operator resource type (e.g. cluster, policy, schedule, user)   delete pgo delete cluster mycluster Delete an Operator resource type (e.g. cluster, policy, user, schedule)   ls pgo ls mycluster Perform a Linux ls command on the cluster.   cat pgo cat mycluster Perform a Linux ls command on the cluster.   df pgo df mycluster Display the disk status/capacity of a Postgres cluster.   failover pgo failover mycluster Perform a manual failover of a Postgres cluster.   help pgo help Display general pgo help information.   label pgo label mycluster \u0026ndash;label=environment=prod Create a metadata label for a Postgres cluster(s).   load pgo load \u0026ndash;load-config=load.json \u0026ndash;selector=name=mycluster Perform a data load into a Postgres cluster(s).   reload pgo reload mycluster Perform a pg_ctl reload command on a Postgres cluster(s).   restore pgo restore mycluster Perform a pgbackrest or pgdump restore on a Postgres cluster.   scale pgo scale mycluster Create a Postgres replica(s) for a given Postgres cluster.   scaledown pgo scaledown mycluster \u0026ndash;query Delete a replica from a Postgres cluster.   show pgo show cluster mycluster Display Operator resource information (e.g. cluster, user, policy, schedule).   status pgo status Display Operator status.   test pgo test mycluster Perform a SQL test on a Postgres cluster(s).   update pgo update cluster \u0026ndash;label=autofail=false Update a Postgres cluster(s).   upgrade pgo upgrade mycluster Perform a minor upgrade to a Postgres cluster(s).   user pgo user \u0026ndash;selector=name=mycluster \u0026ndash;update-passwords Perform Postgres user maintenance on a Postgres cluster(s).   version pgo version Display Operator version information.    Common Operations In all the examples below, the user is specifying the pgouser1 namespace as the target of the operator. Replace this value with your own namespace value. You can specify a default namespace to be used by setting the PGO_NAMESPACE environment variable on the pgo client environment.\nCluster Operations A user will typically start using the Operator by creating a Postgres cluster as follows:\npgo create cluster mycluster -n pgouser1  This command creates a Postgres cluster in the pgouser1 namespace that has a single Postgres primary container.\nYou can see the Postgres cluster using the following:\npgo show cluster mycluster -n pgouser1  You can test the Postgres cluster by entering:\npgo test mycluster -n pgouser1  You can optionally add a Postgres replica to your Postgres cluster as follows:\npgo scale mycluster -n pgouser1  You can create a Postgres cluster initially with a Postgres replica as follows:\npgo create cluster mycluster --replica-count=1 -n pgouser1  To view the Postgres logs, you can enter commands such as:\npgo ls mycluster -n pgouser1 /pgdata/mycluster/pg_log pgo cat mycluster -n pgouser1 /pgdata/mycluster/pg_log/postgresql-Mon.log | tail -3  Backups By default the Operator deploys pgbackrest for a Postgres cluster to hold database backup data.\nYou can create a pgbackrest backup job as follows:\npgo backup mycluster -n pgouser1  You can perform a pgbasebackup job as follows:\npgo backup mycluster --backup-type=pgbasebackup -n pgouser1  You can optionally pass pgbackrest command options into the backup command as follows:\npgo backup mycluster --backup-type=pgbackrest --backup-opts=\u0026quot;--type=diff\u0026quot; -n pgouser1  See pgbackrest.org for command flag descriptions.\nYou can create a Postgres cluster that does not include pgbackrest if you specify the following:\npgo create cluster mycluster --pgbackrest=false -n pgouser1  Scaledown a Cluster You can remove a Postgres replica using the following:\npgo scaledown mycluster --query -n pgouser1 pgo scaledown mycluster --target=sometarget -n pgouser1  Delete a Cluster You can remove a Postgres cluster by entering:\npgo delete cluster mycluster -n pgouser1  Delete a Cluster and Its Persistent Volume Claims You can remove the persistent volumes when removing a Postgres cluster by specifying the following command flag:\npgo delete cluster mycluster --delete-data -n pgouser1  View Disk Utilization You can see a comparison of Postgres data size versus the Persistent volume claim size by entering the following:\npgo df mycluster -n pgouser1  Label Operations Apply a Label to a Cluster You can apply a Kubernetes label to a Postgres cluster as follows:\npgo label mycluster --label=environment=prod -n pgouser1  In this example, the label key is environment and the label value is prod.\nYou can apply labels across a category of Postgres clusters by using the \u0026ndash;selector command flag as follows:\npgo label --selector=clustertypes=research --label=environment=prod -n pgouser1  In this example, any Postgres cluster with the label of clustertypes=research will have the label environment=prod set.\nIn the following command, you can also view Postgres clusters by using the \u0026ndash;selector command flag which specifies a label key value to search with:\npgo show cluster --selector=environment=prod -n pgouser1  Policy Operations Create a Policy To create a SQL policy, enter the following:\npgo create policy mypolicy --in-file=mypolicy.sql -n pgouser1  This examples creates a policy named mypolicy using the contents of the file mypolicy.sql which is assumed to be in the current directory.\nYou can view policies as following:\npgo show policy --all -n pgouser1  Apply a Policy pgo apply mypolicy --selector=environment=prod pgo apply mypolicy --selector=name=mycluster  Operator Status Show Operator Version To see what version of the Operator client and server you are using, enter:\npgo version  To see the Operator server status, enter:\npgo status -n pgouser1  To see the Operator server configuration, enter:\npgo show config -n pgouser1  To see what namespaces exist and if you have access to them, enter:\npgo show namespaces -n pgouser1  Perform a pgdump backup pgo backup mycluster --backup-type=pgdump -n pgouser1 pgo backup mycluster --backup-type=pgdump --backup-opts=\u0026quot;--dump-all --verbose\u0026quot; -n pgouser1 pgo backup mycluster --backup-type=pgdump --backup-opts=\u0026quot;--schema=myschema\u0026quot; -n pgouser1  Note: To run pgdump_all instead of pgdump, pass \u0026lsquo;\u0026ndash;dump-all\u0026rsquo; flag in \u0026ndash;backup-opts as shown above. All \u0026ndash;backup-opts should be space delimited.\nPerform a pgbackrest restore pgo restore mycluster -n pgouser1  Or perform a restore based on a point in time:\npgo restore mycluster --pitr-target=\u0026quot;2019-01-14 00:02:14.921404+00\u0026quot; --backup-opts=\u0026quot;--type=time\u0026quot; -n pgouser1  You can also set the any of the pgbackrest restore options :\npgo restore mycluster --pitr-target=\u0026quot;2019-01-14 00:02:14.921404+00\u0026quot; --backup-opts=\u0026quot; see pgbackrest options \u0026quot; -n pgouser1  You can also target specific nodes when performing a restore:\npgo restore mycluster --node-label=failure-domain.beta.kubernetes.io/zone=us-central1-a -n pgouser1  Here are some steps to test PITR:\n pgo create cluster mycluster create a table on the new cluster called beforebackup pgo backup mycluster -n pgouser1 create a table on the cluster called afterbackup execute select now() on the database to get the time, use this timestamp minus a couple of minutes when you perform the restore pgo restore mycluster \u0026ndash;pitr-target=\u0026ldquo;2019-01-14 00:02:14.921404+00\u0026rdquo; \u0026ndash;backup-opts=\u0026ldquo;\u0026ndash;type=time \u0026ndash;log-level-console=info\u0026rdquo; -n pgouser1 wait for the database to be restored execute \\d in the database and you should see the database state prior to where the afterbackup table was created  See the Design section of the Operator documentation for things to consider before you do a restore.\nRestore from pgbasebackup You can find available pgbasebackup backups to use for a pgbasebackup restore using the pgo show backup command:\n$ pgo show backup mycluster --backup-type=pgbasebackup -n pgouser1 | grep \u0026quot;Backup Path\u0026quot; Backup Path: mycluster-backups/2019-05-21-09-53-20 Backup Path: mycluster-backups/2019-05-21-06-58-50 Backup Path: mycluster-backups/2019-05-21-09-52-52  You can then perform a restore using any available backup path:\npgo restore mycluster --backup-type=pgbasebackup --backup-path=mycluster/2019-05-21-06-58-50 --backup-pvc=mycluster-backup -n pgouser1  When performing the restore, both the backup path and backup PVC can be omitted, and the Operator will use the last pgbasebackup backup created, along with the PVC utilized for that backup:\npgo restore mycluster --backup-type=pgbasebackup -n pgouser1  Once the pgbasebackup restore is complete, a new PVC will be available with a randomly generated ID that contains the restored database, e.g. PVC mycluster-ieqe in the output below:\n$ pgo show pvc all All Operator Labeled PVCs mycluster mycluster-backup mycluster-ieqe  A new cluster can then be created with the same name as the new PVC, as well with the secrets from the original cluster, in order to deploy a new cluster using the restored database:\npgo create cluster mycluster-ieqe --secret-from=mycluster  If you would like to control the name of the PVC created when performing a pgbasebackup restore, use the --restore-to-pvc flag:\npgo restore mycluster --backup-type=pgbasebackup --restore-to-pvc=mycluster-restored -n pgouser1  Restore from pgdump backup pgo restore mycluster --backup-type=pgdump --backup-pvc=mycluster-pgdump-pvc --pitr-target=\u0026quot;2019-01-15-00-03-25\u0026quot; -n pgouser1  To restore the most recent pgdump at the default path, leave off a timestamp:\npgo restore mycluster --backup-type=pgdump --backup-pvc=mycluster-pgdump-pvc -n pgouser1  Fail-over Operations To perform a manual failover, enter the following:\npgo failover mycluster --query -n pgouser1  That example queries to find the available Postgres replicas that could be promoted to the primary.\npgo failover mycluster --target=sometarget -n pgouser1  That command chooses a specific target, and starts the failover workflow.\nCreate a Cluster with Auto-fail Enabled To support an automated failover, you can specify the \u0026ndash;autofail flag on a Postgres cluster when you create it as follows:\npgo create cluster mycluster --autofail --replica-count=1 -n pgouser1  You can set the auto-fail flag on a Postgres cluster after it is created by the following command:\npgo update cluster --label=autofail=false -n pgouser1 pgo update cluster --label=autofail=true -n pgouser1  Note that if you do a pgbackrest restore, you will need to reset the autofail flag to true after the restore is completed.\nAdd-On Operations To add a pgbouncer Deployment to your Postgres cluster, enter:\npgo create cluster mycluster --pgbouncer -n pgouser1  You can add pgbouncer after a Postgres cluster is created as follows:\npgo create pgbouncer mycluster pgo create pgbouncer --selector=name=mycluster  You can also specify a pgbouncer password as follows:\npgo create cluster mycluster --pgbouncer --pgbouncer-pass=somepass -n pgouser1  Note, the pgbouncer configuration defaults to specifying only a single entry for the primary database. If you want it to have an entry for the replica service, add the following configuration to pgbouncer.ini:\n{{.PG_REPLICA_SERVICE_NAME}} = host={{.PG_REPLICA_SERVICE_NAME}} port={{.PG_PORT}} auth_user={{.PG_USERNAME}} dbname={{.PG_DATABASE}}  To add a pgpool Deployment to your Postgres cluster, enter:\npgo create cluster mycluster --pgpool -n pgouser1  You can also add a pgpool to a cluster after initial creation as follows:\npgo create pgpool mycluster -n pgouser1  You can remove a pgbouncer or pgpool from a cluster as follows:\npgo delete pgbouncer mycluster -n pgouser1 pgo delete pgpool mycluster -n pgouser1  You can create a pgbadger sidecar container in your Postgres cluster pod as follows:\npgo create cluster mycluster --pgbadger -n pgouser1  Likewise, you can add the Crunchy Collect Metrics sidecar container into your Postgres cluster pod as follows:\npgo create cluster mycluster --metrics -n pgouser1  Note: backend metric storage such as Prometheus and front end visualization software such as Grafana are not created automatically by the PostgreSQL Operator. For instructions on installing Grafana and Prometheus in your environment, see the Crunchy Container Suite documentation.\nScheduled Tasks There is a cron based scheduler included into the Operator Deployment by default.\nYou can create automated full pgBackRest backups every Sunday at 1 am as follows:\npgo create schedule mycluster --schedule=\u0026quot;0 1 * * SUN\u0026quot; \\ --schedule-type=pgbackrest --pgbackrest-backup-type=full -n pgouser1  You can create automated diff pgBackRest backups every Monday-Saturday at 1 am as follows:\npgo create schedule mycluster --schedule=\u0026quot;0 1 * * MON-SAT\u0026quot; \\ --schedule-type=pgbackrest --pgbackrest-backup-type=diff -n pgouser1  You can create automated pgBaseBackup backups every day at 1 am as follows:\nIn order to have a backup PVC created, users should run the pgo backup command against the target cluster prior to creating this schedule.\npgo create schedule mycluster --schedule=\u0026quot;0 1 * * *\u0026quot; \\ --schedule-type=pgbasebackup --pvc-name=mycluster-backup -n pgouser1  You can create automated Policy every day at 1 am as follows:\npgo create schedule --selector=pg-cluster=mycluster --schedule=\u0026quot;0 1 * * *\u0026quot; \\ --schedule-type=policy --policy=mypolicy --database=userdb \\ --secret=mycluster-testuser-secret -n pgouser1  Benchmark Clusters The pgbench utility containerized and made available to Operator users.\nTo create a Benchmark via Cluster Name you enter:\npgo benchmark mycluster -n pgouser1  To create a Benchmark via Selector, enter:\npgo benchmark --selector=pg-cluster=mycluster -n pgouser1  To create a Benchmark with a custom transactions, enter:\npgo create policy --in-file=/tmp/transactions.sql mytransactions -n pgouser1 pgo benchmark mycluster --policy=mytransactions -n pgouser1  To create a Benchmark with custom parameters, enter:\npgo benchmark mycluster --clients=10 --jobs=2 --scale=10 --transactions=100 -n pgouser1  You can view benchmarks by entering:\npgo show benchmark -n pgouser1  Complex Deployments Create a Cluster using Specific Storage pgo create cluster mycluster --storage-config=somestorageconfig -n pgouser1  Likewise, you can specify a storage configuration when creating a replica:\npgo scale mycluster --storage-config=someslowerstorage -n pgouser1  This example specifies the somestorageconfig storage configuration to be used by the Postgres cluster. This lets you specify a storage configuration that is defined in the pgo.yaml file specifically for a given Postgres cluster.\nYou can create a Cluster using a Preferred Node as follows:\npgo create cluster mycluster --node-label=speed=superfast -n pgouser1  That command will cause a node affinity rule to be added to the Postgres pod which will influence the node upon which Kubernetes will schedule the Pod.\nLikewise, you can create a Replica using a Preferred Node as follows:\npgo scale mycluster --node-label=speed=slowerthannormal -n pgouser1  Create a Cluster with LoadBalancer ServiceType pgo create cluster mycluster --service-type=LoadBalancer -n pgouser1  This command will cause the Postgres Service to be of a specific type instead of the default ClusterIP service type.\nMiscellaneous Create a cluster using the Crunchy Postgres + PostGIS container image:\npgo create cluster mygiscluster --ccp-image=crunchy-postgres-gis -n pgouser1  Create a cluster with a Custom ConfigMap:\npgo create cluster mycustomcluster --custom-config myconfigmap -n pgouser1  pgo Global Flags pgo global command flags include:\n   Flag Description     n namespace targeted for the command   apiserver-url URL of the Operator REST API service, override with CO_APISERVER_URL environment variable   debug enable debug messages   pgo-ca-cert The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. Override with PGO_CA_CERT environment variable   pgo-client-cert The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. Override with PGO_CLIENT_CERT environment variable   pgo-client-key The Client Key file path for authenticating to the PostgreSQL Operator apiserver. Override with PGO_CLIENT_KEY environment variable    pgo Global Environment Variables pgo will pick up these settings if set in your environment:\n| Name | Description | NOTES | |PGOUSERNAME |The username (role) used for auth on the operator apiserver. | Requires that PGOUSERPASS be set. | |PGOUSERPASS |The password for used for auth on the operator apiserver. | Requires that PGOUSERNAME be set. | |PGOUSER |The path the the pgorole file. | Will be ignored if either PGOUSERNAME or PGOUSERPASS are set. |\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/",
	"title": "Operator Command Line Utility",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/gettingstarted/design/olm/",
	"title": "OLM",
	"tags": [],
	"description": "",
	"content": " Operator Lifecycle Management The Postgres Operator supports Redhats OLM (operator lifecycle manager) to a degree starting with pgo 4.0.\nThe Postgres Operator supports the different deployment models as documented here:\nhttps://github.com/operator-framework/operator-lifecycle-manager/blob/master/Documentation/design/operatorgroups.md\nOperator Hub The Operator shows up on the Redhat Operator Hub at the following location:\nhttps://www.operatorhub.io/operator/postgres-operator.v3.5.0\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/security/",
	"title": "Security",
	"tags": [],
	"description": "",
	"content": " Kubernetes RBAC Install the requisite Operator RBAC resources, as a Kubernetes cluster admin user, by running a Makefile target:\nmake installrbac  This script creates the following RBAC resources on your Kubernetes cluster:\n   Setting Definition     Custom Resource Definitions (crd.yaml) pgbackups    pgclusters    pgpolicies    pgreplicas    pgtasks    pgupgrades   Cluster Roles (cluster-roles.yaml) pgopclusterrole    pgopclusterrolecrd   Cluster Role Bindings (cluster-roles-bindings.yaml) pgopclusterbinding    pgopclusterbindingcrd   Service Account (service-accounts.yaml) postgres-operator    pgo-backrest   Roles (rbac.yaml) pgo-role    pgo-backrest-role   Role Bindings (rbac.yaml) pgo-backrest-role-binding    pgo-role-binding    Note that the cluster role bindings have a naming convention of pgopclusterbinding-$PGO_OPERATOR_NAMESPACE and pgopclusterbindingcrd-$PGO_OPERATOR_NAMESPACE. The PGO_OPERATOR_NAMESPACE environment variable is added to make each cluster role binding name unique and to support more than a single Operator being deployed on the same Kube cluster.\nOperator RBAC The conf/postgresql-operator/pgorole file is read at start up time when the operator is deployed to the Kubernetes cluster. This file defines the Operator roles whereby Operator API users can be authorized.\nThe conf/postgresql-operator/pgouser file is read at start up time also and contains username, password, role, and namespace information as follows:\nusername:password:pgoadmin: pgouser1:password:pgoadmin:pgouser1 pgouser2:password:pgoadmin:pgouser2 pgouser3:password:pgoadmin:pgouser1,pgouser2 readonlyuser:password:pgoreader:  The format of the pgouser server file is:\n\u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;:\u0026lt;role\u0026gt;:\u0026lt;namespace,namespace\u0026gt;  The namespace is a comma separated list of namespaces that user has access to. If you do not specify a namespace, then all namespaces is assumed, meaning this user can access any namespace that the Operator is watching.\nA user creates a .pgouser file in their $HOME directory to identify themselves to the Operator. An entry in .pgouser will need to match entries in the conf/postgresql-operator/pgouser file. A sample .pgouser file contains the following:\nusername:password  The format of the .pgouser client file is:\n\u0026lt;username\u0026gt;:\u0026lt;password\u0026gt;  The users pgouser file can also be located at: /etc/pgo/pgouser or it can be found at a path specified by the PGOUSER environment variable.\nIf the user tries to access a namespace that they are not configured for within the server side pgouser file then they will get an error message as follows:\nError: user [pgouser1] is not allowed access to namespace [pgouser2]  The following list shows the current complete list of possible pgo permissions that you can specify within the pgorole file when creating roles:\n   Permission Description     ApplyPolicy allow pgo apply   Cat allow pgo cat   CreateBackup allow pgo backup   CreateBenchmark allow pgo create benchmark   CreateCluster allow pgo create cluster   CreateDump allow pgo create pgdump   CreateFailover allow pgo failover   CreatePgbouncer allow pgo create pgbouncer   CreatePgpool allow pgo create pgpool   CreatePolicy allow pgo create policy   CreateSchedule allow pgo create schedule   CreateUpgrade allow pgo upgrade   CreateUser allow pgo create user   DeleteBackup allow pgo delete backup   DeleteBenchmark allow pgo delete benchmark   DeleteCluster allow pgo delete cluster   DeletePgbouncer allow pgo delete pgbouncer   DeletePgpool allow pgo delete pgpool   DeletePolicy allow pgo delete policy   DeleteSchedule allow pgo delete schedule   DeleteUpgrade allow pgo delete upgrade   DeleteUser allow pgo delete user   DfCluster allow pgo df   Label allow pgo label   Load allow pgo load   Ls allow pgo ls   Reload allow pgo reload   Restore allow pgo restore   RestoreDump allow pgo restore for pgdumps   ShowBackup allow pgo show backup   ShowBenchmark allow pgo show benchmark   ShowCluster allow pgo show cluster   ShowConfig allow pgo show config   ShowPolicy allow pgo show policy   ShowPVC allow pgo show pvc   ShowSchedule allow pgo show schedule   ShowNamespace allow pgo show namespace   ShowUpgrade allow pgo show upgrade   ShowWorkflow allow pgo show workflow   Status allow pgo status   TestCluster allow pgo test   UpdateCluster allow pgo update cluster   User allow pgo user   Version allow pgo version    If the user is unauthorized for a pgo command, the user will get back this response:\nError: Authentication Failed: 401  Making Security Changes The Operator today requires you to make Operator user security changes in the pgouser and pgorole files, and for those changes to take effect you are required to re-deploy the Operator:\nmake deployoperator  This will recreate the pgo-config ConfigMap that stores these files and is mounted by the Operator during its initialization.\nAPI Security The Operator REST API is encrypted with keys stored in the pgo.tls Secret.\nThe pgo.tls Secret can be generated prior to starting the Operator or you can let the Operator generate the Secret for you if the Secret does not exist.\nAdjust the default keys to meet your security requirements using your own keys. The pgo.tls Secret is created when you run:\nmake deployoperator  The keys are generated when the RBAC script is executed by the cluster admin:\nmake installrbac  In some scenarios like an OLM deployment, it is preferable for the Operator to generate the Secret keys at runtime, if the pgo.tls Secret does not exit when the Operator starts, a new TLS Secret will be generated. In this scenario, you can extract the generated Secret TLS keys using:\nkubectl cp \u0026lt;pgo-namespace\u0026gt;/\u0026lt;pgo-pod\u0026gt;:/tmp/server.key /tmp/server.key -c apiserver kubectl cp \u0026lt;pgo-namespace\u0026gt;/\u0026lt;pgo-pod\u0026gt;:/tmp/server.crt /tmp/server.crt -c apiserver  example of the command below:\nkubectl cp pgo/postgres-operator-585584f57d-ntwr5:tmp/server.key /tmp/server.key -c apiserver kubectl cp pgo/postgres-operator-585584f57d-ntwr5:tmp/server.crt /tmp/server.crt -c apiserver  This server.key and server.crt can then be used to access the pgo-apiserver from the pgo CLI by setting the following variables in your client environment:\nexport PGO_CA_CERT=/tmp/server.crt export PGO_CLIENT_CERT=/tmp/server.crt export PGO_CLIENT_KEY=/tmp/server.key  You can view the TLS secret using:\nkubectl get secret pgo.tls -n pgo  or\noc get secret pgo.tls -n pgo  If you create the Secret outside of the Operator, for example using the default installation script, the key and cert that are generated by the default installation are found here:\n$PGOROOT/conf/postgres-operator/server.crt $PGOROOT/conf/postgres-operator/server.key  The key and cert are generated using the deploy/gen-api-keys.sh script. That script gets executed when running:\nmake installrbac  You can extract the server.key and server.crt from the Secret using the following:\noc get secret pgo.tls -n $PGO_OPERATOR_NAMESPACE -o jsonpath='{.data.tls\\.key}' | base64 --decode \u0026gt; /tmp/server.key oc get secret pgo.tls -n $PGO_OPERATOR_NAMESPACE -o jsonpath='{.data.tls\\.crt}' | base64 --decode \u0026gt; /tmp/server.crt  This server.key and server.crt can then be used to access the pgo-apiserver REST API from the pgo CLI on your client host.\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/upgrade/",
	"title": "Upgrade",
	"tags": [],
	"description": "",
	"content": " Upgrading the Operator Various Operator releases will require action by the Operator administrator of your organization in order to upgrade to the next release of the Operator. Some upgrade steps are automated within the Operator but not all are possible at this time.\nThis section of the documentation shows specific steps required to the latest version from the previous version.\nUpgrading to Version 3.5.0 From Previous Versions  For clusters created in prior versions that used pgbackrest, you will be required to first create a pgbasebackup for those clusters, and after upgrading to Operator 3.5, you will need to restore those clusters from the pgbasebackup into a new cluster with \u0026ndash;pgbackrest enabled, this is due to the new pgbackrest shared repository being implemented in 3.5. This is a breaking change for anyone that used pgbackrest in Operator versions prior to 3.5. The pgingest CRD is removed. You will need to manually remove it from any deployments of the operator after upgrading to this version. This includes removing ingest related permissions from the pgorole file. Additionally, the API server now removes the ingest related API endpoints. Primary and replica labels are only applicable at cluster creation and are not updated after a cluster has executed a failover. A new service-name label is applied to PG cluster components to indicate whether a deployment/pod is a primary or replica. service-name is also the label now used by the cluster services to route with. This scheme allows for an almost immediate failover promotion and avoids the pod having to be bounced as part of a failover. Any existing PostgreSQL clusters will need to be updated to specify them as a primary or replica using the new service-name labeling scheme.\n The autofail label was moved from deployments and pods to just the pgcluster CRD to support autofail toggling. The storage configurations in pgo.yaml support the MatchLabels attribute for NFS storage. This will allow users to have more than a single NFS backend,. When set, this label (key=value) will be used to match the labels on PVs when a PVC is created. The UpdateCluster permission was added to the sample pgorole file to support the new pgo update CLI command. It was also added to the pgoperm file. The pgo.yaml adds the PreferredFailoverNode setting. This is a Kubernetes selector string (e.g. key=value). This value if set, will cause fail-over targets to be preferred based on the node they run on if that node is in the set of preferred. The ability to select nodes based on a selector string was added. For this to feature to be used, multiple replicas have to be in a ready state, and also at the same replication status. If those conditions are not met, the default fail-over target selection is used. The pgo.yaml file now includes a new storage configuration, XlogStorage, which when set will cause the xlog volume to be allocated using this storage configuration. If not set, the PrimaryStorage configuration will be used. The pgo.yaml file now includes a new storage configuration, BackrestStorage, will cause the pgbackrest shared repository volume to be allocated using this storage configuration. The pgo.yaml file now includes a setting, AutofailReplaceReplica, which will enable or disable whether a new replica is created as part of a fail-over. This is turned off by default.  See the GitHub Release notes for the features and other notes about a specific release.\nUpgrading a Cluster from Version 3.5.x to 4.0 This section will outline the procedure to upgrade a given cluster created using Postgres Operator 3.5.x to 4.0.\n1) Create a new Centos/Redhat user with the same permissions are the existing user used to install the Crunchy Postgres Operator. This is necessary to avoid any issues with environment variable differences between 3.5 and 4.0.\n2) For the cluster(s) you wish to upgrade, scale down any replicas, if necessary, then delete the cluster\npgo delete cluster \u0026lt;clustername\u0026gt;  IMPORTANT NOTES: Please note the name of each cluster, the namespace used, and be sure not to delete the associated PVCs or CRDs!\n3) Delete the 3.5.x version of the operator by executing:\n$COROOT/deploy/cleanup.sh $COROOT/deploy/remove-crd.sh  4) Log in as your new Linux user and install the 4.0 Postgres Operator. Be sure to add the existing namespace to the list of watched namespaces (see the \u0026lsquo;Getting Started-\u0026gt;Design-\u0026gt;Namespace\u0026rsquo; section of this document for more information) and make sure to avoid overwriting any existing data storage.\n5) Once the Operator is installed and functional, create a new 4.0 cluster with the same name as was used previously. This will allow the new cluster to utilize the existing PVCs.\npgo create cluster \u0026lt;clustername\u0026gt; -n \u0026lt;namespace\u0026gt;  6) Manually update the old leftover Secrets to use the new label as defined in 4.0:\nkubectl label secret/\u0026lt;clustername\u0026gt;-postgres-secret pg-cluster=\u0026lt;clustername\u0026gt; -n \u0026lt;namespace\u0026gt; kubectl label secret/\u0026lt;clustername\u0026gt;-primaryuser-secret pg-cluster=\u0026lt;clustername\u0026gt; -n \u0026lt;namespace\u0026gt; kubectl label secret/\u0026lt;clustername\u0026gt;-testuser-secret pg-cluster=\u0026lt;clustername\u0026gt; -n \u0026lt;namespace\u0026gt;  7) To verify cluster status, run pgo test  -n  Output should be similar to:\npsql -p 5432 -h 10.104.74.189 -U postgres postgres is Working psql -p 5432 -h 10.104.74.189 -U postgres userdb is Working psql -p 5432 -h 10.104.74.189 -U primaryuser postgres is Working psql -p 5432 -h 10.104.74.189 -U primaryuser userdb is Working psql -p 5432 -h 10.104.74.189 -U testuser postgres is Working psql -p 5432 -h 10.104.74.189 -U testuser userdb is Working  8) Scale up to the required number of replicas, as needed.\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/contributing/",
	"title": "Contributing",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/",
	"title": "Operator CLI",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/install-with-ansible/prerequisites/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": " Prerequisites The following is required prior to installing Crunchy PostgreSQL Operator using Ansible:\n postgres-operator playbooks source code for the target version Ansible 2.4.6+  Kubernetes Installs  Kubernetes v1.11+ Cluster admin privileges in Kubernetes kubectl configured to communicate with Kubernetes  OpenShift Installs  OpenShift v3.09+ Cluster admin privileges in OpenShift oc configured to communicate with OpenShift  Installing from a Windows Host If the Crunchy PostgreSQL Operator is being installed from a Windows host the following are required:\n Windows Subsystem for Linux (WSL) Ubuntu for Windows  Permissions The installation of the Crunchy PostgreSQL Operator requires elevated privileges.\nIt is required that the playbooks are run as a cluster-admin to ensure the playbooks can install:\n Custom Resource Definitions Cluster RBAC Create required namespaces  Obtaining Operator Ansible Role There are two ways to obtain the Crunchy PostgreSQL Operator Roles:\n Cloning the postgres-operator project postgres-operator-playbooks RPM provided for Crunchy customers via the Crunchy Access Portal.  GitHub Installation All necessary files (inventory, main playbook and roles) can be found in the ansible directory in the postgres-operator project.\nRPM Installation using Yum Available to Crunchy customers is an RPM containing all the necessary Ansible roles and files required for installation using Ansible. The RPM can be found in Crunchy\u0026rsquo;s yum repository. For information on setting up yum to use the Crunchy repoistory, see the Crunchy Access Portal.\nTo install the Crunchy PostgreSQL Operator Ansible roles using yum, run the following command on a RHEL or CentOS host:\nsudo yum install postgres-operator-playbooks  Ansible roles can be found in: /usr/share/ansible/roles/crunchydata Ansible playbooks/inventory files can be found in: /usr/share/ansible/postgres-operator/playbooks  Once installed users should take a copy of the inventory file included in the installation using the following command:\ncp /usr/share/ansible/postgres-operator/playbooks/inventory ${HOME?} Configuring the Inventory File The inventory file included with the PostgreSQL Operator Playbooks allows installers to configure how the operator will function when deployed into Kubernetes. This file should contain all configurable variables the playbooks offer.\nThe following are the variables available for configuration:\n   Name Default Description     archive_mode true Set to true enable archive logging on all newly created clusters.   archive_timeout 60 Set to a value in seconds to configure the timeout threshold for archiving.   auto_failover_replace_replica false Set to true to replace promoted replicas during failovers with a new replica on all newly created clusters.   auto_failover_sleep_secs 9 Set to a value in seconds to configure the sleep time before initiating a failover on all newly created clusters.   auto_failover false Set to true enable auto failover capabilities on all newly created cluster requests. This can be disabled by the client.   backrest false Set to true enable pgBackRest capabilities on all newly created cluster request. This can be disabled by the client.   backrest_aws_s3_key  Set to configure the key used by pgBackRest to authenticate with Amazon Web Service S3 for backups and restoration in S3.   backrest_aws_s3_secret  Set to configure the secret used by pgBackRest to authenticate with Amazon Web Service S3 for backups and restoration in S3.   backrest_aws_s3_bucket  Set to configure the bucket used by pgBackRest with Amazon Web Service S3 for backups and restoration in S3.   backrest_aws_s3_endpoint  Set to configure the endpoint used by pgBackRest with Amazon Web Service S3 for backups and restoration in S3.   backrest_aws_s3_region  Set to configure the region used by pgBackRest with Amazon Web Service S3 for backups and restoration in S3.   backrest_storage storage1 Set to configure which storage definition to use when creating volumes used by pgBackRest on all newly created clusters.   badger false Set to true enable pgBadger capabilities on all newly created clusters. This can be disabled by the client.   ccp_image_prefix crunchydata Configures the image prefix used when creating containers from Crunchy Container Suite.   ccp_image_tag  Configures the image tag (version) used when creating containers from Crunchy Container Suite.   cleanup false Set to configure the playbooks to delete all objects when deprovisioning Operator. Note: this will delete all objects related to the Operator (including clusters provisioned).   crunchy_debug false Set to configure Operator to use debugging mode. Note: this can cause sensitive data such as passwords to appear in Operator logs.   db_name userdb Set to a value to configure the default database name on all newly created clusters.   db_password_age_days 60 Set to a value in days to configure the expiration age on PostgreSQL role passwords on all newly created clusters.   db_password_length 20 Set to configure the size of passwords generated by the operator on all newly created roles.   db_port 5432 Set to configure the default port used on all newly created clusters.   db_replicas 1 Set to configure the amount of replicas provisioned on all newly created clusters.   db_user testuser Set to configure the username of the dedicated user account on all newly created clusters.   grafana_admin_username admin Set to configure the login username for the Grafana administrator.   grafana_admin_password  Set to configure the login password for the Grafana administrator.   grafana_install true Set to true to install Crunchy Grafana to visualize metrics.   grafana_storage_access_mode  Set to the access mode used by the configured storage class for Grafana persistent volumes.   grafana_storage_class_name  Set to the name of the storage class used when creating Grafana persistent volumes.   grafana_volume_size  Set to the size of persistent volume to create for Grafana.   kubernetes_context  When deploying to Kubernetes, set to configure the context name of the kubeconfig to be used for authentication.   log_statement none Set to none, ddl, mod, or all to configure the statements that will be logged in PostgreSQL\u0026rsquo;s logs on all newly created clusters.   metrics false Set to true enable performance metrics on all newly created clusters. This can be disabled by the client.   metrics_namespace metrics Configures the target namespace when deploying Grafana and/or Prometheus   namespace  Set to a comma delimited string of all the namespaces Operator will manage.   openshift_host  When deploying to OpenShift, set to configure the hostname of the OpenShift cluster to connect to.   openshift_password  When deploying to OpenShift, set to configure the password used for login.   openshift_skip_tls_verify  When deploying to Openshift, set to ignore the integrity of TLS certificates for the OpenShift cluster.   openshift_token  When deploying to OpenShift, set to configure the token used for login (when not using username/password authentication).   openshift_user  When deploying to OpenShift, set to configure the username used for login.   pgo_admin_username admin Configures the pgo administrator username.   pgo_admin_password  Configures the pgo administrator password.   pgo_client_install true Configures the playbooks to install the pgo client if set to true.   pgo_client_version  Configures which version of pgo the playbooks should install.   pgo_image_prefix crunchydata Configures the image prefix used when creating containers for the Crunchy PostgreSQL Operator (apiserver, operator, scheduler..etc).   pgo_image_tag  Configures the image tag used when creating containers for the Crunchy PostgreSQL Operator (apiserver, operator, scheduler..etc)   pgo_operator_namespace  Set to configure the namespace where Operator will be deployed.   pgo_tls_no_verify  Set to configure Operator to verify TLS certificates.   primary_storage storage2 Set to configure which storage definition to use when creating volumes used by PostgreSQL primaries on all newly created clusters.   prometheus_install true Set to true to install Crunchy Prometheus timeseries database.   prometheus_storage_access_mode  Set to the access mode used by the configured storage class for Prometheus persistent volumes.   prometheus_storage_class_name  Set to the name of the storage class used when creating Prometheus persistent volumes.   replica_storage storage3 Set to configure which storage definition to use when creating volumes used by PostgreSQL replicas on all newly created clusters.   scheduler_timeout 3600 Set to a value in seconds to configure the pgo-scheduler timeout threshold when waiting for schedules to complete.   service_type ClusterIP Set to configure the type of Kubernetes service provisioned on all newly created clusters.   storage\u0026lt;ID\u0026gt;_access_mode  Set to configure the access mode of the volumes created when using this storage definition.   storage\u0026lt;ID\u0026gt;_class  Set to configure the storage class name used when creating dynamic volumes.   storage\u0026lt;ID\u0026gt;_fs_group  Set to configure any filesystem groups that should be added to security contexts on newly created clusters.   storage\u0026lt;ID\u0026gt;_size  Set to configure the size of the volumes created when using this storage definition.   storage\u0026lt;ID\u0026gt;_supplemental_groups  Set to configure any supplemental groups that should be added to security contexts on newly created clusters.   storage\u0026lt;ID\u0026gt;_type  Set to either create or dynamic to configure the operator to create persistent volumes or have them created dynamically by a storage class.    To retrieve the `kubernetes_context` value for Kubernetes installs, run the following command: ```bash kubectl config current-context ```  Minimal Variable Requirements The following variables should be configured at a minimum to deploy the Crunchy PostgreSQL Operator:\n backrest_storage ccp_image_prefix ccp_image_tag kubernetes_context namespace openshift_host openshift_password openshift_skip_tls_verify openshift_token openshift_user pgo_admin_username pgo_admin_password pgo_client_install pgo_image_prefix pgo_image_tag pgo_operator_namespace pgo_tls_no_verify primary_storage replica_storage storage\u0026lt;ID\u0026gt;_access_mode storage\u0026lt;ID\u0026gt;_class storage\u0026lt;ID\u0026gt;_fs_group storage\u0026lt;ID\u0026gt;_size storage\u0026lt;ID\u0026gt;_supplemental_groups storage\u0026lt;ID\u0026gt;_type  Users should remove or comment out the `kubernetes` or `openshift` variables if they're not being used from the inventory file. Both sets of variables cannot be used at the same time.  Storage Kubernetes and OpenShift offer support for a variety of storage types. The Crunchy PostgreSQL Operator must be configured to utilize the storage options available by configuring the storage options included in the inventory file.\nAt this time the Crunchy PostgreSQL Operator Playbooks only support storage classes. For more information on storage classes see the [official Kubernetes documentation](https://kubernetes.io/docs/concepts/storage/storage-classes/).  Considerations for Multi-Zone Cloud Environments When using the Operator in a Kubernetes cluster consisting of nodes that span multiple zones, special consideration must betaken to ensure all pods and the volumes they require are scheduled and provisioned within the same zone. Specifically, being that a pod is unable mount a volume that is located in another zone, any volumes that are dynamically provisioned must be provisioned in a topology-aware manner according to the specific scheduling requirements for the pod. For instance, this means ensuring that the volume containing the database files for the primary database in a new PostgreSQL cluster is provisioned in the same zone as the node containing the PostgreSQL primary pod that will be using it.\nFor instructions on setting up storage classes for multi-zone environments, see the PostgreSQL Operator Documentation.\nExamples The following are examples on configuring the storage variables for different types of storage classes.\nGeneric Storage Class To setup storage1 to use the storage class fast\nstorage1_access_mode=\u0026#39;ReadWriteOnce\u0026#39; storage1_size=\u0026#39;10G\u0026#39; storage1_type=\u0026#39;dynamic\u0026#39; storage1_class=\u0026#39;fast\u0026#39; To assign this storage definition to all primary pods created by the Operator, we can configure the primary_storage=storage1 variable in the inventory file.\nGKE The storage class provided by Google Kubernetes Environment (GKE) can be configured to be used by the Operator by setting the following variables in the inventory file:\nstorage1_access_mode=\u0026#39;ReadWriteOnce\u0026#39; storage1_size=\u0026#39;10G\u0026#39; storage1_type=\u0026#39;dynamic\u0026#39; storage1_class=\u0026#39;standard\u0026#39; storage1_fs_group=26 To assign this storage definition to all primary pods created by the Operator, we can configure the primary_storage=storage1 variable in the inventory file.\nTo utitlize mutli-zone deployments, see _Considerations for Multi-Zone Cloud Environments_ above.  Understanding pgo_operator_namespace \u0026amp; namespace The Crunchy PostgreSQL Operator can be configured to be deployed and manage a single namespace or manage several namespaces. The following are examples of different types of deployment models configurable in the inventory file.\nSingle Namespace To deploy the Crunchy PostgreSQL Operator to work with a single namespace (in this example our namespace is named pgo), configure the following inventory settings:\npgo_operator_namespace=\u0026#39;pgo\u0026#39; namespace=\u0026#39;pgo\u0026#39; Multiple Namespaces To deploy the Crunchy PostgreSQL Operator to work with multiple namespaces (in this example our namespaces are named pgo, pgouser1 and pgouser2), configure the following inventory settings:\npgo_operator_namespace=\u0026#39;pgo\u0026#39; namespace=\u0026#39;pgouser1,pgouser2\u0026#39; Deploying Multiple Operators The 4.0 release of the Crunchy PostgreSQL Operator allows for multiple operator deployments in the same cluster.\nTo install the Crunchy PostgreSQL Operator to multiple namespaces, it\u0026rsquo;s recommended to have an inventory file for each deployment of the operator.\nFor each operator deployment the following inventory variables should be configured uniquely for each install.\nFor example, operator could be deployed twice by changing the pgo_operator_namespace and namespace for those deployments:\nInventory A would deploy operator to the pgo namespace and it would manage the pgo target namespace.\n# Inventory A pgo_operator_namespace=\u0026#39;pgo\u0026#39; namespace=\u0026#39;pgo\u0026#39; ... Inventory B would deploy operator to the pgo2 namespace and it would manage the pgo2 and pgo3 target namespaces.\n# Inventory B pgo_operator_namespace=\u0026#39;pgo2\u0026#39; namespace=\u0026#39;pgo2,pgo3\u0026#39; ... Each install of the operator will create a corresponding directory in $HOME/.pgo/\u0026lt;PGO NAMESPACE\u0026gt; which will contain the TLS and pgouser client credentials.\nDeploying Grafana and Prometheus PostgreSQL clusters created by the operator can be configured to create additional containers for collecting metrics.\nThese metrics are very useful for understanding the overall health and performance of PostgreSQL database deployments over time. The collectors included by the operator are:\n Node Exporter - Host metrics where the PostgreSQL containers are running PostgreSQL Exporter - PostgreSQL metrics  The operator, however, does not install the necessary timeseries database (Prometheus) for storing the collected metrics or the front end visualization (Grafana) of those metrics.\nIncluded in these playbooks are roles for deploying Granfana and/or Prometheus. See the inventory file for options to install the metrics stack.\nAt this time the Crunchy PostgreSQL Operator Playbooks only support storage classes.  "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/install-with-ansible/installing-ansible/",
	"title": "Installing Ansible",
	"tags": [],
	"description": "",
	"content": " Installing Ansible on Linux, MacOS or Windows Ubuntu Subsystem To install Ansible on Linux or MacOS, see the official documentation provided by Ansible.\nInstall Google Cloud SDK (Optional) If Crunchy PostgreSQL Operator is going to be installed in a Google Kubernetes Environment the Google Cloud SDK is required.\nTo install the Google Cloud SDK on Linux or MacOS, see the official Google Cloud documentation.\nWhen installing the Google Cloud SDK on the Windows Ubuntu Subsystem, run the following commands to install:\nwget https://sdk.cloud.google.com --output-document=/tmp/install-gsdk.sh # Review the /tmp/install-gsdk.sh prior to running chmod +x /tmp/install-gsdk.sh /tmp/install-gsdk.sh"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/gettingstarted/prereq/",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": " Prerequisites The following is required prior to installing PostgreSQL Operator:\n Kubernetes v1.13+ Red Hat OpenShift v3.11+ VMWare Enterprise PKS 1.3+ kubectl or oc configured to communicate with Kubernetes  "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/install-with-ansible/installing-operator/",
	"title": "Installing PostgreSQL Operator",
	"tags": [],
	"description": "",
	"content": " Installing The following assumes the proper prerequisites are satisfied we can now install the PostgreSQL Operator.\nThe commands should be run in the directory where the Crunchy PostgreSQL Operator playbooks is stored. See the ansible directory in the Crunchy PostgreSQL Operator project for the inventory file, main playbook and ansible roles.\nInstalling on Linux On a Linux host with Ansible installed we can run the following command to install the PostgreSQL Operator:\nansible-playbook -i /path/to/inventory --tags=install --ask-become-pass main.yml If the Crunchy PostgreSQL Operator playbooks were installed using yum, use the following commands:\nexport ANSIBLE_ROLES_PATH=/usr/share/ansible/roles/crunchydata ansible-playbook -i /path/to/inventory --tags=install --ask-become-pass \\  /usr/share/ansible/postgres-operator/playbooks/main.yml Installing on MacOS On a MacOS host with Ansible installed we can run the following command to install the PostgreSQL Operator.\nansible-playbook -i /path/to/inventory --tags=install --ask-become-pass main.yml Installing on Windows Ubuntu Subsystem On a Windows host with an Ubuntu subsystem we can run the following commands to install the PostgreSQL Operator.\nansible-playbook -i /path/to/inventory --tags=install --ask-become-pass main.yml Verifying the Installation This may take a few minutes to deploy. To check the status of the deployment run the following:\n# Kubernetes kubectl get deployments -n \u0026lt;NAMESPACE_NAME\u0026gt; kubectl get pods -n \u0026lt;NAMESPACE_NAME\u0026gt; # OpenShift oc get deployments -n \u0026lt;NAMESPACE_NAME\u0026gt; oc get pods -n \u0026lt;NAMESPACE_NAME\u0026gt; Configure Environment Variables After the Crunchy PostgreSQL Operator has successfully been installed we will need to configure local environment variables before using the pgo client.\nTo configure the environment variables used by pgo run the following command:\nNote: \u0026lt;PGO_NAMESPACE\u0026gt; should be replaced with the namespace the Crunchy PostgreSQL Operator was deployed to.\ncat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; ~/.bashrc export PGOUSER=\u0026#34;${HOME?}/.pgo/\u0026lt;PGO_NAMESPACE\u0026gt;/pgouser\u0026#34; export PGO_CA_CERT=\u0026#34;${HOME?}/.pgo/\u0026lt;PGO_NAMESPACE\u0026gt;/client.crt\u0026#34; export PGO_CLIENT_CERT=\u0026#34;${HOME?}/.pgo/\u0026lt;PGO_NAMESPACE\u0026gt;/client.crt\u0026#34; export PGO_CLIENT_KEY=\u0026#34;${HOME?}/.pgo/\u0026lt;PGO_NAMESPACE\u0026gt;/client.pem\u0026#34; export PGO_APISERVER_URL=\u0026#39;https://127.0.0.1:8443\u0026#39; EOF Apply those changes to the current session by running:\nsource ~/.bashrc Verify pgo Connection In a separate terminal we need to setup a port forward to the Crunchy PostgreSQL Operator to ensure connection can be made outside of the cluster:\n# If deployed to Kubernetes kubectl port-forward \u0026lt;OPERATOR_POD_NAME\u0026gt; -n \u0026lt;OPERATOR_NAMESPACE\u0026gt; 8443:8443 # If deployed to OpenShift oc port-forward \u0026lt;OPERATOR_POD_NAME\u0026gt; -n \u0026lt;OPERATOR_NAMESPACE\u0026gt; 8443:8443 On a separate terminal verify the pgo can communicate with the Crunchy PostgreSQL Operator:\npgo version If the above command outputs versions of both the client and API server, the Crunchy PostgreSQL Operator has been installed successfully.\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/install-with-ansible/installing-metrics/",
	"title": "Installing Metrics Infrastructure",
	"tags": [],
	"description": "",
	"content": " Installing PostgreSQL clusters created by the Crunchy PostgreSQL Operator can optionally be configured to serve performance metrics via Prometheus Exporters. The metric exporters included in the database pod serve realtime metrics for the database container. In order to store and view this data, Grafana and Prometheus are required. The Crunchy PostgreSQL Operator does not create this infrastructure, however, they can be installed using the provided Ansible roles.\nPrerequisites The following assumes the proper prerequisites are satisfied we can now install the PostgreSQL Operator.\nAt a minimum, the following inventory variables should be configured to install the metrics infrastructure:\n   Name Default Description     ccp_image_prefix crunchydata Configures the image prefix used when creating containers from Crunchy Container Suite.   ccp_image_tag  Configures the image tag (version) used when creating containers from Crunchy Container Suite.   grafana_admin_username admin Set to configure the login username for the Grafana administrator.   grafana_admin_password  Set to configure the login password for the Grafana administrator.   grafana_install true Set to true to install Crunchy Grafana to visualize metrics.   grafana_storage_access_mode  Set to the access mode used by the configured storage class for Grafana persistent volumes.   grafana_storage_class_name  Set to the name of the storage class used when creating Grafana persistent volumes.   grafana_volume_size  Set to the size of persistent volume to create for Grafana.   kubernetes_context  When deploying to Kubernetes, set to configure the context name of the kubeconfig to be used for authentication.   metrics false Set to true enable performance metrics on all newly created clusters. This can be disabled by the client.   metrics_namespace metrics Configures the target namespace when deploying Grafana and/or Prometheus   openshift_host  When deploying to OpenShift, set to configure the hostname of the OpenShift cluster to connect to.   openshift_password  When deploying to OpenShift, set to configure the password used for login.   openshift_skip_tls_verify  When deploying to Openshift, set to ignore the integrity of TLS certificates for the OpenShift cluster.   openshift_token  When deploying to OpenShift, set to configure the token used for login (when not using username/password authentication).   openshift_user  When deploying to OpenShift, set to configure the username used for login.   prometheus_install true Set to true to install Crunchy Prometheus timeseries database.   prometheus_storage_access_mode  Set to the access mode used by the configured storage class for Prometheus persistent volumes.   prometheus_storage_class_name  Set to the name of the storage class used when creating Prometheus persistent volumes.    Administrators can choose to install Grafana, Prometheus or both by configuring the `grafana_install` and `prometheus_install` variables in the inventory files.  The following commands should be run in the directory where the Crunchy PostgreSQL Operator playbooks are located. See the ansible directory in the Crunchy PostgreSQL Operator project for the inventory file, main playbook and ansible roles.\nAt this time the Crunchy PostgreSQL Operator Playbooks only support storage classes. For more information on storage classes see the [official Kubernetes documentation](https://kubernetes.io/docs/concepts/storage/storage-classes/).  Installing on Linux On a Linux host with Ansible installed we can run the following command to install the Metrics stack:\nansible-playbook -i /path/to/inventory --tags=install-metrics main.yml If the Crunchy PostgreSQL Operator playbooks were installed using yum, use the following commands:\nexport ANSIBLE_ROLES_PATH=/usr/share/ansible/roles/crunchydata ansible-playbook -i /path/to/inventory --tags=install-metrics --ask-become-pass \\  /usr/share/ansible/postgres-operator/playbooks/main.yml Installing on MacOS On a MacOS host with Ansible installed we can run the following command to install the Metrics stack:\nansible-playbook -i /path/to/inventory --tags=install-metrics main.yml Installing on Windows On a Windows host with the Ubuntu subsystem we can run the following commands to install the Metrics stack:\nansible-playbook -i /path/to/inventory --tags=install-metrics main.yml Verifying the Installation This may take a few minutes to deploy. To check the status of the deployment run the following:\n# Kubernetes kubectl get deployments -n \u0026lt;NAMESPACE_NAME\u0026gt; kubectl get pods -n \u0026lt;NAMESPACE_NAME\u0026gt; # OpenShift oc get deployments -n \u0026lt;NAMESPACE_NAME\u0026gt; oc get pods -n \u0026lt;NAMESPACE_NAME\u0026gt; Verify Grafana In a separate terminal we need to setup a port forward to the Crunchy Grafana deployment to ensure connection can be made outside of the cluster:\n# If deployed to Kubernetes kubectl port-forward \u0026lt;GRAFANA_POD_NAME\u0026gt; -n \u0026lt;METRICS_NAMESPACE\u0026gt; 3000:3000 # If deployed to OpenShift oc port-forward \u0026lt;GRAFANA_POD_NAME\u0026gt; -n \u0026lt;METRICS_NAMESPACE\u0026gt; 3000:3000 In a browser navigate to https://127.0.0.1:3000 to access the Grafana dashboard.\nNo metrics will be scraped if no exporters are available. To create a PostgreSQL cluster with metric exporters run the following command: ```bash pgo create cluster --metrics --namespace= ```  Verify Prometheus In a separate terminal we need to setup a port forward to the Crunchy Prometheus deployment to ensure connection can be made outside of the cluster:\n# If deployed to Kubernetes kubectl port-forward \u0026lt;PROMETHEUS_POD_NAME\u0026gt; -n \u0026lt;METRICS_NAMESPACE\u0026gt; 9090:9090 # If deployed to OpenShift oc port-forward \u0026lt;PROMETHEUS_POD_NAME\u0026gt; -n \u0026lt;METRICS_NAMESPACE\u0026gt; 9090:9090 In a browser navigate to https://127.0.0.1:9090 to access the Prometheus dashboard.\nNo metrics will be scraped if no exporters are available. To create a PostgreSQL cluster with metric exporters run the following command: ```bash pgo create cluster --metrics --namespace= ```  "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/install-with-ansible/updating-operator/",
	"title": "Updating PostgreSQL Operator",
	"tags": [],
	"description": "",
	"content": " Updating Updating the Crunchy PostgreSQL Operator is essential to the lifecycle management of the service. Using the update flag will:\n Update and redeploy the operator deployment Recreate configuration maps used by operator Remove any deprecated objects Allow administrators to change settings configured in the inventory Reinstall the pgo client if a new version is specified  The following assumes the proper prerequisites are satisfied we can now update the PostgreSQL Operator.\nThe commands should be run in the directory where the Crunchy PostgreSQL Operator playbooks is stored. See the ansible directory in the Crunchy PostgreSQL Operator project for the inventory file, main playbook and ansible roles.\nUpdating on Linux On a Linux host with Ansible installed we can run the following command to update\nthe PostgreSQL Operator:\nansible-playbook -i /path/to/inventory --tags=update --ask-become-pass main.yml If the Crunchy PostgreSQL Operator playbooks were installed using yum, use the following commands:\nexport ANSIBLE_ROLES_PATH=/usr/share/ansible/roles/crunchydata ansible-playbook -i /path/to/inventory --tags=update --ask-become-pass \\  /usr/share/ansible/postgres-operator/playbooks/main.yml Updating on MacOS On a MacOS host with Ansible installed we can run the following command to update\nthe PostgreSQL Operator.\nansible-playbook -i /path/to/inventory --tags=update --ask-become-pass main.yml Updating on Windows Ubuntu Subsystem On a Windows host with an Ubuntu subsystem we can run the following commands to update\nthe PostgreSQL Operator.\nansible-playbook -i /path/to/inventory --tags=update --ask-become-pass main.yml Verifying the Update This may take a few minutes to deploy. To check the status of the deployment run the following:\n# Kubernetes kubectl get deployments -n \u0026lt;NAMESPACE_NAME\u0026gt; kubectl get pods -n \u0026lt;NAMESPACE_NAME\u0026gt; # OpenShift oc get deployments -n \u0026lt;NAMESPACE_NAME\u0026gt; oc get pods -n \u0026lt;NAMESPACE_NAME\u0026gt; Configure Environment Variables After the Crunchy PostgreSQL Operator has successfully been updated we will need to configure local environment variables before using the pgo client.\nTo configure the environment variables used by pgo run the following command:\nNote: \u0026lt;PGO_NAMESPACE\u0026gt; should be replaced with the namespace the Crunchy PostgreSQL Operator was deployed to.\ncat \u0026lt;\u0026lt;EOF \u0026gt;\u0026gt; ~/.bashrc export PGOUSER=\u0026#34;${HOME?}/.pgo/\u0026lt;PGO_NAMESPACE\u0026gt;/pgouser\u0026#34; export PGO_CA_CERT=\u0026#34;${HOME?}/.pgo/\u0026lt;PGO_NAMESPACE\u0026gt;/client.crt\u0026#34; export PGO_CLIENT_CERT=\u0026#34;${HOME?}/.pgo/\u0026lt;PGO_NAMESPACE\u0026gt;/client.crt\u0026#34; export PGO_CLIENT_KEY=\u0026#34;${HOME?}/.pgo/\u0026lt;PGO_NAMESPACE\u0026gt;/client.pem\u0026#34; export PGO_APISERVER_URL=\u0026#39;https://127.0.0.1:8443\u0026#39; EOF Apply those changes to the current session by running:\nsource ~/.bashrc Verify pgo Connection In a separate terminal we need to setup a port forward to the Crunchy PostgreSQL Operator to ensure connection can be made outside of the cluster:\n# If deployed to Kubernetes kubectl port-forward \u0026lt;OPERATOR_POD_NAME\u0026gt; -n \u0026lt;OPERATOR_NAMESPACE\u0026gt; 8443:8443 # If deployed to OpenShift oc port-forward \u0026lt;OPERATOR_POD_NAME\u0026gt; -n \u0026lt;OPERATOR_NAMESPACE\u0026gt; 8443:8443 On a separate terminal verify the pgo can communicate with the Crunchy PostgreSQL Operator:\npgo version If the above command outputs versions of both the client and API server, the Crunchy PostgreSQL Operator has been updated successfully.\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/install-with-ansible/uninstalling-operator/",
	"title": "Uninstalling PostgreSQL Operator",
	"tags": [],
	"description": "",
	"content": " Uninstalling PostgreSQL Operator The following assumes the proper prerequisites are satisfied we can now deprovision the PostgreSQL Operator.\nFirst, it is recommended to use the playbooks tagged with the same version of the PostgreSQL Operator currently deployed.\nWith the correct playbooks acquired and prerequisites satisfied, simply run the following command:\nansible-playbook -i /path/to/inventory --tags=deprovision --ask-become-pass main.yml If the Crunchy PostgreSQL Operator playbooks were installed using yum, use the following commands:\nexport ANSIBLE_ROLES_PATH=/usr/share/ansible/roles/crunchydata ansible-playbook -i /path/to/inventory --tags=deprovision --ask-become-pass \\  /usr/share/ansible/postgres-operator/playbooks/main.yml Deleting pgo Client If variable pgo_client_install is set to true in the inventory file, the pgo client will also be uninstalled when deprovisioning.\nOtherwise, the pgo client can be manually uninstalled by running the following command:\nrm /usr/local/bin/pgo  "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/install-with-ansible/uninstalling-metrics/",
	"title": "Uninstalling Metrics Stack",
	"tags": [],
	"description": "",
	"content": " Uninstalling the Metrics Stack The following assumes the proper prerequisites are satisfied we can now deprovision the PostgreSQL Operator Metrics Infrastructure.\nFirst, it is recommended to use the playbooks tagged with the same version of the Metrics stack currently deployed.\nWith the correct playbooks acquired and prerequisites satisfied, simply run the following command:\nansible-playbook -i /path/to/inventory --tags=deprovision-metrics main.yml If the Crunchy PostgreSQL Operator playbooks were installed using yum, use the following commands:\nexport ANSIBLE_ROLES_PATH=/usr/share/ansible/roles/crunchydata ansible-playbook -i /path/to/inventory --tags=deprovision-metrics \\  /usr/share/ansible/postgres-operator/playbooks/main.yml"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/operator-install/",
	"title": "Install Operator Using Bash",
	"tags": [],
	"description": "",
	"content": " A full installation of the Operator includes the following steps:\n create a project structure configure your environment variables configure Operator templates create security resources deploy the operator install pgo CLI (end user command tool)  Operator end-users are only required to install the pgo CLI client on their host and can skip the server-side installation steps. pgo CLI clients are provided on the Github Releases page for Linux, Mac, and Windows clients.\nThe Operator can be deployed by multiple methods including:\n default installation Ansible playbook installation Openshift Console installation using OLM  Default Installation - Create Project Structure The Operator follows a golang project structure, you can create a structure as follows on your local Linux host:\nmkdir -p $HOME/odev/src/github.com/crunchydata $HOME/odev/bin $HOME/odev/pkg cd $HOME/odev/src/github.com/crunchydata git clone https://github.com/CrunchyData/postgres-operator.git cd postgres-operator git checkout 4.0.0  This creates a directory structure under your HOME directory name odev and clones the current Operator version to that structure.\nDefault Installation - Configure Environment Environment variables control aspects of the Operator installation. You can copy a sample set of Operator environment variables and aliases to your .bashrc file to work with.\ncat $HOME/odev/src/github.com/crunchydata/postgres-operator/examples/envs.sh \u0026gt;\u0026gt; $HOME/.bashrc source $HOME/.bashrc  For various scripts used by the Operator, the expenv utility is required, download this utility from the Github Releases page, and place it into your PATH (e.g. $HOME/odev/bin). There is also a Makefile target that includes is expenv and several other dependencies that are only needed if you plan on building from source:\nmake setup  \nDefault Installation - Namespace Creation The default installation will create 3 namespaces to use for deploying the Operator into and for holding Postgres clusters created by the Operator.\nCreating Kube namespaces is typically something that only a priviledged Kube user can perform so log into your Kube cluster as a user that has the necessary priviledges.\nOn Openshift if you do not want to install the Operator as the system administrator, you can grant cluster-admin priviledges to a user as follows:\noc adm policy add-cluster-role-to-user cluster-admin pgoinstaller  In the above command, you are granting cluster-admin priviledges to a user named pgoinstaller.\nThe NAMESPACE environment variable is a comma separated list of namespaces that specify where the Operator will be provisioing PG clusters into, specifically, the namespaces the Operator is watching for Kube events. This value is set as follows:\nexport NAMESPACE=pgouser1,pgouser2  This means namespaces called pgouser1 and pgouser2 will be created as part of the default installation.\nThe PGO_OPERATOR_NAMESPACE environment variable is a comma separated list of namespace values that the Operator itself will be deployed into. For the installation example, this value is set as follows:\nexport PGO_OPERATOR_NAMESPACE=pgo  This means a pgo namespace will be created and the Operator will be deployed into that namespace.\nCreate the Operator namespaces using the Makefile target:\nmake setupnamespaces  The Design section of this documentation talks further about the use of namespaces within the Operator.\nDefault Installation - Configure Operator Templates Within the Operator conf directory are several configuration files and templates used by the Operator to determine the various resources that it deploys on your Kubernetes cluster, specifically the PostgreSQL clusters it deploys.\nWhen you install the Operator you must make choices as to what kind of storage the Operator has to work with for example. Storage varies with each installation. As an installer, you would modify these configuration templates used by the Operator to customize its behavior.\nNote: when you want to make changes to these Operator templates and configuration files after your initial installation, you will need to re-deploy the Operator in order for it to pick up any future configuration changes.\nHere are some common examples of configuration changes most installers would make:\nStorage Inside conf/postgresql-operator/pgo.yaml there are various storage configurations defined.\nPrimaryStorage: gce XlogStorage: gce ArchiveStorage: gce BackupStorage: gce ReplicaStorage: gce gce: AccessMode: ReadWriteOnce Size: 1G StorageType: dynamic StorageClass: standard Fsgroup: 26  Listed above are the pgo.yaml sections related to storage choices. PrimaryStorage specifies the name of the storage configuration used for PostgreSQL primary database volumes to be provisioned. In the example above, a NFS storage configuration is picked. That same storage configuration is selected for the other volumes that the Operator will create.\nThis sort of configuration allows for a PostgreSQL primary and replica to use different storage if you want. Other storage settings like AccessMode, Size, StorageType, StorageClass, and Fsgroup further define the storage configuration. Currently, NFS, HostPath, and Storage Classes are supported in the configuration.\nAs part of the Operator installation, you will need to adjust these storage settings to suit your deployment requirements. For users wanting to try out the Operator on Google Kubernetes Engine you would make the following change to the storage configuration in pgo.yaml:\nFor NFS Storage, it is assumed that there are sufficient Persistent Volumes (PV) created for the Operator to use when it creates Persistent Volume Claims (PVC). The creation of Persistent Volumes is something a Kubernetes cluster-admin user would typically provide before installing the Operator. There is an example script which can be used to create NFS Persistent Volumes located here:\n./pv/create-nfs-pv.sh  That script looks for the IP address of an NFS server using the environment variable PGO_NFS_IP you would set in your .bashrc environment.\nA similar script is provided for HostPath persistent volume creation if you wanted to use HostPath for testing:\n./pv/create-pv.sh  Adjust the above PV creation scripts to suit your local requirements, the purpose of these scripts are solely to produce a test set of Volume to test the Operator.\nOther settings in pgo.yaml are described in the pgo.yaml Configuration section of the documentation.\nOperator Security The Operator implements its own RBAC (Role Based Access Controls) for authenticating Operator users access to the Operator REST API.\nThere is a default set of Roles and Users defined respectively in the following files and can be copied into your home directory as such:\n./conf/postgres-operator/pgouser ./conf/postgres-operator/pgorole cp ./conf/postgres-operator/pgouser $HOME/.pgouser cp ./conf/postgres-operator/pgorole $HOME/.pgorole  Or create a .pgouser file in your home directory with a credential known by the Operator (see your administrator for Operator credentials to use):\n username:password or pgouser1:password or pgouser2:password or pgouser3:password or readonlyuser:password  Each example above has different priviledges in the Operator. You can create this file as follows:\necho \u0026quot;pgouser3:password\u0026quot; \u0026gt; $HOME/.pgouser  Note, you can also store the pgouser file in alternate locations, see the Security documentation for details.\nOperator security is discussed in the Security section Security of the documentation.\nAdjust these settings to meet your local requirements.\nDefault Installation - Create Kube RBAC Controls The Operator installation requires Kubernetes administrators to create Resources required by the Operator. These resources are only allowed to be created by a cluster-admin user. To install on Google Cloud, you will need a user account with cluster-admin priviledges. If you own the GKE cluster you are installing on, you can add cluster-admin role to your account as follows:\nkubectl create clusterrolebinding cluster-admin-binding --clusterrole cluster-admin --user $(gcloud config get-value account)  Specifically, Custom Resource Definitions for the Operator, and Service Accounts used by the Operator are created which require cluster permissions.\nTor create the Kube RBAC used by the Operator, run the following as a cluster-admin Kube user:\nmake installrbac  This set of Resources is created a single time unless a new Operator release requires these Resources to be recreated. Note that when you run make installrbac the set of keys used by the Operator REST API and also the pgbackrest ssh keys are generated.\nVerify the Operator Custom Resource Definitions are created as follows:\nkubectl get crd  You should see the pgclusters CRD among the listed CRD resource types.\nSee the Security documentation for a description of the various RBAC resources created and used by the Operator.\nDefault Installation - Deploy the Operator At this point, you as a normal Kubernetes user should be able to deploy the Operator. To do this, run the following Makefile target:\nmake deployoperator  This will cause any existing Operator to be removed first, then the configuration to be bundled into a ConfigMap, then the Operator Deployment to be created.\nThis will create a postgres-operator Deployment and a postgres-operator Service.Operator administrators needing to make changes to the Operator configuration would run this make target to pick up any changes to pgo.yaml, pgo users/roles, or the Operator templates.\nDefault Installation - Completely Cleaning Up You can completely remove all the namespaces you have previously created using the default installation by running the following:\nmake cleannamespaces  This will permanently delete each namespace the Operator installation created previously.\npgo CLI Installation Most users will work with the Operator using the pgo CLI tool. That tool is downloaded from the GitHub Releases page for the Operator (https://github.com/crunchydata/postgres-operator/releases).\nThe pgo client is provided in Mac, Windows, and Linux binary formats, download the appropriate client to your local laptop or workstation to work with a remote Operator.\nPrior to using pgo, users testing the Operator on a single host can specify the postgres-operator URL as follows:\n $ kubectl get service postgres-operator -n pgo NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE postgres-operator 10.104.47.110 \u0026lt;none\u0026gt; 8443/TCP 7m $ export PGO_APISERVER_URL=https://10.104.47.110:8443 pgo version  That URL address needs to be reachable from your local pgo client host. Your Kubernetes administrator will likely need to create a network route, ingress, or LoadBalancer service to expose the Operator REST API to applications outside of the Kubernetes cluster. Your Kubernetes administrator might also allow you to run the Kubernetes port-forward command, contact your adminstrator for details.\nNext, the pgo client needs to reference the keys used to secure the Operator REST API:\n export PGO_CA_CERT=$PGOROOT/conf/postgres-operator/server.crt export PGO_CLIENT_CERT=$PGOROOT/conf/postgres-operator/server.crt export PGO_CLIENT_KEY=$PGOROOT/conf/postgres-operator/server.key  You can also specify these keys on the command line as follows:\npgo version --pgo-ca-cert=$PGOROOT/conf/postgres-operator/server.crt --pgo-client-cert=$PGOROOT/conf/postgres-operator/server.crt --pgo-client-key=$PGOROOT/conf/postgres-operator/server.key  if you are running the Operator on Google Cloud, you would open up another terminal and run *kubectl port-forward ...* to forward the Operator pod port 8443 to your localhost where you can access the Operator API from your local workstation.  At this point, you can test connectivity between your laptop or workstation and the Postgres Operator deployed on a Kubernetes cluster as follows:\npgo version  You should get back a valid response showing the client and server version numbers.\nVerify the Installation Now that you have deployed the Operator, you can verify that it is running correctly.\nYou should see a pod running that contains the Operator:\nkubectl get pod --selector=name=postgres-operator -n pgo NAME READY STATUS RESTARTS AGE postgres-operator-79bf94c658-zczf6 3/3 Running 0 47s  That pod should show 3 of 3 containers in running state and that the operator is installed into the pgo namespace.\nThe sample environment script, examples/env.sh, if used creates some bash functions that you can use to view the Operator logs. This is useful in case you find one of the Operator containers not in a running status.\nUsing the pgo CLI, you can verify the versions of the client and server match as follows:\npgo version  This also tests connectivity between your pgo client host and the Operator server.\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/installation/developer-setup/",
	"title": "Developer Setup",
	"tags": [],
	"description": "",
	"content": " Developing The Postgres-Operator is an open source project hosted on GitHub.\nDevelopers that wish to build the Operator from source or contribute to the project via pull requests would set up a development environment through the following steps.\nCreate Kubernetes Cluster We use either OpenShift Container Platform or kubeadm to install development clusters.\nCreate a Local Development Host We currently build on CentOS 7 and RHEL 7 hosts. Others operating systems are possible, however we do not support building or running the Operator on other operating systems at this time.\nPerform Manual Install You can follow the manual installation method described in this documentation to make sure you can deploy from your local development host to your Kubernetes cluster.\nBuild Locally You can now build the Operator from source on local on your development host. Here are some steps to follow:\nGet Build Dependencies Run the following target to install a golang compiler, and any other build dependencies:\nmake setup  Compile You will build all the Operator binaries and Docker images by running:\nmake all  This assumes you have Docker installed and running on your development host.\nThe project uses the golang dep package manager to vendor all the golang source dependencies into the vendor directory. You typically do not need to run any dep commands unless you are adding new golang package dependencies into the project outside of what is within the project for a given release.\nAfter a full compile, you will have a pgo binary in $HOME/odev/bin and the Operator images in your local Docker registry.\nRelease You can perform a release build by running:\nmake release  This will compile the Mac and Windows versions of pgo.\nDeploy Now that you have built the Operator images, you can push them to your Kubernetes cluster if that cluster is remote to your development host.\nYou would then run:\nmake deployoperator  To deploy the Operator on your Kubernetes cluster. If your Kubernetes cluster is not local to your development host, you will need to specify a config file that will connect you to your Kubernetes cluster. See the Kubernetes documentation for details.\nDebug Debug level logging in turned on by default when deploying the Operator.\nSample bash functions are supplied in examples/envs.sh to view the Operator logs.\nYou can view the Operator REST API logs with the alog bash function.\nYou can view the Operator core logic logs with the olog bash function.\nYou can view the Scheduler logs with the slog bash function.\nYou can enable the pgo CLI debugging with the following flag:\npgo version --debug  You can set the REST API URL as follows after a deployment if you are developing on your local host by executing the setip bash function.\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/contributing/documentation-updates/",
	"title": "Updating Documentation",
	"tags": [],
	"description": "",
	"content": " Documentation The documentation website is generated using Hugo.\nHosting Hugo Locally (Optional) If you would like to build the documentation locally, view the official Installing Hugo guide to set up Hugo locally.\nYou can then start the server by running the following commands -\ncd $COROOT/hugo/ hugo server  The local version of the Hugo server is accessible by default from localhost:1313. Once you\u0026rsquo;ve run hugo server, that will let you interactively make changes to the documentation as desired and view the updates in real-time.\nContributing to the Documentation All documentation is in Markdown format and uses Hugo weights for positioning of the pages.\nThe current production release documentation is updated for every tagged major release.\nWhen you\u0026rsquo;re ready to commit a change, please verify that the documentation generates locally.\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/contributing/issues/",
	"title": "Submitting Issues",
	"tags": [],
	"description": "",
	"content": "If you would like to submit an feature / issue for us to consider please submit an to the official GitHub Repository.\nIf you would like to work the issue, please add that information in the issue so that we can confirm we are not already working no need to duplicate efforts.\nIf you have any question you can submit a Support - Question and Answer issue and we will work with you on how you can get more involved.\n"
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/contributing/pull-requests/",
	"title": "Submitting Pull Requests",
	"tags": [],
	"description": "",
	"content": "So you decided to submit an issue and work it. Great! Let\u0026rsquo;s get it merged in to the codebase. The following will go a long way to helping get the fix merged in quicker.\n Create a Pull Request from your Fork to the Develop branch. Update the checklists in the Pull Request Description. Reference which issues this Pull Request is resolving.  "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/categories/",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/",
	"title": "Crunchy Data Postgres Operator",
	"tags": [],
	"description": "",
	"content": " Latest Release: 4.0.0\nThe postgres-operator is a controller that runs within a Kubernetes cluster that provides a means to deploy and manage PostgreSQL clusters.\nUse the postgres-operator to:\n deploy PostgreSQL containers including streaming replication clusters scale up PostgreSQL clusters with extra replicas add pgpool, pgbouncer, and metrics sidecars to PostgreSQL clusters apply SQL policies to PostgreSQL clusters assign metadata tags to PostgreSQL clusters maintain PostgreSQL users and passwords perform minor upgrades to PostgreSQL clusters load simple CSV and JSON files into PostgreSQL clusters perform database backups  Deployment Requirements The Operator is validated for deployment on Kubernetes, OpenShift, and VMware Enterprise PKS clusters. Some form of storage is required, NFS, HostPath, and Storage Classes are currently supported.\nThe Operator includes various components that get deployed to your Kubernetes cluster as shown in the following diagram and detailed in the Design.\nThe Operator is developed and tested on CentOS and RHEL Linux platforms but is known to run on other Linux variants.\nDocumentation The following documentation is provided:\n pgo CLI Syntax and Examples Installation Configuration pgo.yaml Configuration Security Design Overview Developing Upgrading the Operator Contributing  "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/tags/",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo/",
	"title": "pgo",
	"tags": [],
	"description": "",
	"content": " pgo The pgo command line interface.\nSynopsis The pgo command line interface lets you create and manage PostgreSQL clusters.\nOptions  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -h, --help help for pgo -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo apply - Apply a policy pgo backup - Perform a Backup pgo benchmark - Perform a pgBench benchmark against clusters pgo cat - Perform a cat command on a cluster pgo create - Create a Cluster, PGBouncer, PGPool, Policy, Schedule, or User pgo delete - Delete a backup, benchmark, cluster, pgbouncer, pgpool, label, policy, or user pgo df - Display disk space for clusters pgo failover - Performs a manual failover pgo label - Label a set of clusters pgo load - Perform a data load pgo ls - Perform a ls command on a cluster pgo reload - Perform a cluster reload pgo restore - Perform a restore from previous backup pgo scale - Scale a PostgreSQL cluster pgo scaledown - Scale down a PostgreSQL cluster pgo show - Show the description of a cluster pgo status - Display PostgreSQL cluster status pgo test - Test cluster connectivity pgo update - Update a cluster pgo upgrade - Perform an upgrade pgo user - Manage PostgreSQL users pgo version - Print version information for the PostgreSQL Operator  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_apply/",
	"title": "pgo_apply",
	"tags": [],
	"description": "",
	"content": " pgo apply Apply a policy\nSynopsis APPLY allows you to apply a Policy to a set of clusters. For example:\npgo apply mypolicy1 --selector=name=mycluster pgo apply mypolicy1 --selector=someotherpolicy pgo apply mypolicy1 --selector=someotherpolicy --dry-run  pgo apply [flags]  Options  --dry-run Shows the clusters that the label would be applied to, without labelling them. -h, --help help for apply -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_backup/",
	"title": "pgo_backup",
	"tags": [],
	"description": "",
	"content": " pgo backup Perform a Backup\nSynopsis BACKUP performs a Backup, for example:\npgo backup mycluster\npgo backup [flags]  Options  --backup-opts string The pgbackup options to pass into pgbasebackup or pgbackrest. --backup-type string The backup type to perform. Default is pgbasebackup. Valid backup types are pgbasebackup, pgbackrest and pgdump. (default \u0026quot;pgbackrest\u0026quot;) -h, --help help for backup --pgbackrest-storage-type string The type of storage to use when scheduling pgBackRest backups. Either \u0026quot;local\u0026quot;, \u0026quot;s3\u0026quot; or both, comma separated. (default \u0026quot;local\u0026quot;) --pvc-name string The PVC name to use for the backup instead of the default. -s, --selector string The selector to use for cluster filtering. --storage-config string The name of a Storage config in pgo.yaml to use for the cluster storage. Only applies to pgbasebackup backups.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_benchmark/",
	"title": "pgo_benchmark",
	"tags": [],
	"description": "",
	"content": " pgo benchmark Perform a pgBench benchmark against clusters\nSynopsis Benchmark run pgBench against PostgreSQL clusters, for example:\npgo benchmark mycluster\npgo benchmark [flags]  Options  -b, --benchmark-opts string The extra flags passed to pgBench during the benchmark. -c, --clients int The number of clients to be used in the benchmark. (default 1) -d, --database string The database where the benchmark should be run. (default \u0026quot;postgres\u0026quot;) -h, --help help for benchmark -i, --init-opts string The extra flags passed to pgBench during the initialization of the benchmark. -j, --jobs int The number of worker threads to use for the benchmark. (default 1) -p, --policy string The name of the policy specifying custom transaction SQL for advanced benchmarking. --scale int The number to scale the amount of rows generated for the benchmark. (default 1) -s, --selector string The selector to use for cluster filtering. -t, --transactions int The number of transaction each client should run in the benchmark. (default 1)  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_cat/",
	"title": "pgo_cat",
	"tags": [],
	"description": "",
	"content": " pgo cat Perform a cat command on a cluster\nSynopsis CAT performs a Linux cat command on a cluster file. For example:\npgo cat mycluster /pgdata/mycluster/postgresql.conf  pgo cat [flags]  Options  -h, --help help for cat  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_clidoc/",
	"title": "pgo_clidoc",
	"tags": [],
	"description": "",
	"content": " pgo clidoc Generate Markdown of CLI commandes\nSynopsis The clidoc command allows you to generate markdown files for all CLI commands:\npgo clidoc  pgo clidoc [flags]  Options  -h, --help help for clidoc  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 21-Feb-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_create/",
	"title": "pgo_create",
	"tags": [],
	"description": "",
	"content": " pgo create Create a Cluster, PGBouncer, PGPool, Policy, Schedule, or User\nSynopsis CREATE allows you to create a new Cluster, PGBouncer, PGPool, Policy, Schedule or User. For example:\npgo create cluster pgo create pgbouncer pgo create pgpool pgo create policy pgo create user  pgo create [flags]  Options  -h, --help help for create  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface. pgo create cluster - Create a PostgreSQL cluster pgo create pgbouncer - Create a pgbouncer pgo create pgpool - Create a pgpool pgo create policy - Create a SQL policy pgo create schedule - Create a cron-like scheduled task pgo create user - Create a PostgreSQL user  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_create_cluster/",
	"title": "pgo_create_cluster",
	"tags": [],
	"description": "",
	"content": " pgo create cluster Create a PostgreSQL cluster\nSynopsis Create a PostgreSQL cluster consisting of a primary and a number of replica backends. For example:\npgo create cluster mycluster  pgo create cluster [flags]  Options  --autofail If set, will cause autofailover to be enabled on this cluster. --ccp-image string The CCPImage name to use for cluster creation. If specified, overrides the value crunchy-postgres. -c, --ccp-image-tag string The CCPImageTag to use for cluster creation. If specified, overrides the pgo.yaml setting. --custom-config string The name of a configMap that holds custom PostgreSQL configuration files used to override defaults. -h, --help help for cluster -l, --labels string The labels to apply to this cluster. --metrics Adds the crunchy-collect container to the database pod. --node-label string The node label (key=value) to use in placing the primary database. If not set, any node is used. -w, --password string The password to use for initial database users. --pgbackrest Enables a pgBackRest volume for the database pod. (default true) --pgbackrest-storage-type string The type of storage to use with pgBackRest. Either \u0026quot;local\u0026quot;, \u0026quot;s3\u0026quot; or both, comma separated. (default \u0026quot;local\u0026quot;) --pgbadger Adds the crunchy-pgbadger container to the database pod. --pgbouncer Adds a crunchy-pgbouncer deployment to the cluster. --pgbouncer-pass string Password for the pgbouncer user of the crunchy-pgboucer deployment. --pgpool Adds the crunchy-pgpool container to the database pod. --pgpool-secret string The name of a pgpool secret to use for the pgpool configuration. -z, --policies string The policies to apply when creating a cluster, comma separated. --replica-count int The number of replicas to create as part of the cluster. --replica-storage-config string The name of a Storage config in pgo.yaml to use for the cluster replica storage. -r, --resources-config string The name of a container resource configuration in pgo.yaml that holds CPU and memory requests and limits. -s, --secret-from string The cluster name to use when restoring secrets. -e, --series int The number of clusters to create in a series. (default 1) --service-type string The Service type to use for the PostgreSQL cluster. If not set, the pgo.yaml default will be used. --storage-config string The name of a Storage config in pgo.yaml to use for the cluster storage.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo create - Create a Cluster, PGBouncer, PGPool, Policy, Schedule, or User  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_create_pgbouncer/",
	"title": "pgo_create_pgbouncer",
	"tags": [],
	"description": "",
	"content": " pgo create pgbouncer Create a pgbouncer\nSynopsis Create a pgbouncer. For example:\npgo create pgbouncer mycluster  pgo create pgbouncer [flags]  Options  -h, --help help for pgbouncer --pgbouncer-pass string Password for the pgbouncer user of the crunchy-pgboucer deployment. -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo create - Create a Cluster, PGBouncer, PGPool, Policy, Schedule, or User  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_create_pgpool/",
	"title": "pgo_create_pgpool",
	"tags": [],
	"description": "",
	"content": " pgo create pgpool Create a pgpool\nSynopsis Create a pgpool. For example:\npgo create pgpool mycluster  pgo create pgpool [flags]  Options  -h, --help help for pgpool --pgpool-secret string The name of a pgpool secret to use for the pgpool configuration.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo create - Create a Cluster, PGBouncer, PGPool, Policy, Schedule, or User  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_create_policy/",
	"title": "pgo_create_policy",
	"tags": [],
	"description": "",
	"content": " pgo create policy Create a SQL policy\nSynopsis Create a policy. For example:\npgo create policy mypolicy --in-file=/tmp/mypolicy.sql  pgo create policy [flags]  Options  -h, --help help for policy -i, --in-file string The policy file path to use for adding a policy. -u, --url string The url to use for adding a policy.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo create - Create a Cluster, PGBouncer, PGPool, Policy, Schedule, or User  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_create_schedule/",
	"title": "pgo_create_schedule",
	"tags": [],
	"description": "",
	"content": " pgo create schedule Create a cron-like scheduled task\nSynopsis Schedule creates a cron-like scheduled task. For example:\npgo create schedule --schedule=\u0026quot;* * * * *\u0026quot; --schedule-type=pgbackrest --pgbackrest-backup-type=full mycluster  pgo create schedule [flags]  Options  -c, --ccp-image-tag string The CCPImageTag to use for cluster creation. If specified, overrides the pgo.yaml setting. --database string The database to run the SQL policy against. -h, --help help for schedule --pgbackrest-backup-type string The type of pgBackRest backup to schedule (full or diff). --pgbackrest-storage-type string The type of storage to use when scheduling pgBackRest backups. Either \u0026quot;local\u0026quot;, \u0026quot;s3\u0026quot; or both, comma separated. (default \u0026quot;local\u0026quot;) --policy string The policy to use for SQL schedules. --pvc-name string The name of the backup PVC to use (only used in pgbasebackup schedules). --schedule string The schedule assigned to the cron task. --schedule-opts string The custom options passed to the create schedule API. --schedule-type string The type of schedule to be created (pgbackrest, pgbasebackup or policy). --secret string The secret name for the username and password of the PostgreSQL role for SQL schedules. -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo create - Create a Cluster, PGBouncer, PGPool, Policy, Schedule, or User  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_create_user/",
	"title": "pgo_create_user",
	"tags": [],
	"description": "",
	"content": " pgo create user Create a PostgreSQL user\nSynopsis Create a postgres user. For example:\npgo create user manageduser --managed --selector=name=mycluster pgo create user user1 --selector=name=mycluster  pgo create user [flags]  Options  --db string Grants the user access to a database. -h, --help help for user --managed Creates a user with secrets that can be managed by the Operator. --password string The password to use for creating a new user which overrides a generated password. --password-length int If no password is supplied, this is the length of the auto generated password (default 12) -s, --selector string The selector to use for cluster filtering. --valid-days int Sets passwords for new users to X days. (default 30)  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo create - Create a Cluster, PGBouncer, PGPool, Policy, Schedule, or User  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_delete/",
	"title": "pgo_delete",
	"tags": [],
	"description": "",
	"content": " pgo delete Delete a backup, benchmark, cluster, pgbouncer, pgpool, label, policy, or user\nSynopsis The delete command allows you to delete a backup, benchmark, cluster, label, pgbouncer, pgpool, policy, or user. For example:\npgo delete backup mycluster pgo delete benchmark mycluster pgo delete cluster mycluster pgo delete cluster mycluster --delete-data pgo delete cluster mycluster --delete-data --delete-backups pgo delete label mycluster --label=env=research pgo delete pgbouncer mycluster pgo delete pgpool mycluster pgo delete policy mypolicy pgo delete schedule --schedule-name=mycluster-pgbackrest-full pgo delete schedule --selector=name=mycluster pgo delete schedule mycluster pgo delete user testuser --selector=name=mycluster  pgo delete [flags]  Options  -h, --help help for delete  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface. pgo delete backup - Delete a backup pgo delete benchmark - Delete benchmarks for a cluster pgo delete cluster - Delete a PostgreSQL cluster pgo delete label - Delete a label from clusters pgo delete pgbouncer - Delete a pgbouncer from a cluster pgo delete pgpool - Delete a pgpool from a cluster pgo delete policy - Delete a SQL policy pgo delete schedule - Delete a schedule pgo delete user - Delete a user  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_delete_backup/",
	"title": "pgo_delete_backup",
	"tags": [],
	"description": "",
	"content": " pgo delete backup Delete a backup\nSynopsis Delete a backup. For example:\npgo delete backup mydatabase  pgo delete backup [flags]  Options  -h, --help help for backup  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo delete - Delete a backup, benchmark, cluster, pgbouncer, pgpool, label, policy, or user  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_delete_benchmark/",
	"title": "pgo_delete_benchmark",
	"tags": [],
	"description": "",
	"content": " pgo delete benchmark Delete benchmarks for a cluster\nSynopsis Delete benchmarks for a cluster. For example:\npgo delete benchmark mycluster pgo delete benchmark --selector=env=test  pgo delete benchmark [flags]  Options  -h, --help help for benchmark -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo delete - Delete a backup, benchmark, cluster, pgbouncer, pgpool, label, policy, or user  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_delete_cluster/",
	"title": "pgo_delete_cluster",
	"tags": [],
	"description": "",
	"content": " pgo delete cluster Delete a PostgreSQL cluster\nSynopsis Delete a PostgreSQL cluster. For example:\npgo delete cluster --all pgo delete cluster mycluster  pgo delete cluster [flags]  Options  --all all resources. -b, --delete-backups Causes the backups for this cluster to be removed permanently. -d, --delete-data Causes the data for this cluster to be removed permanently. -h, --help help for cluster --no-prompt No command line confirmation. -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo delete - Delete a backup, benchmark, cluster, pgbouncer, pgpool, label, policy, or user  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_delete_label/",
	"title": "pgo_delete_label",
	"tags": [],
	"description": "",
	"content": " pgo delete label Delete a label from clusters\nSynopsis Delete a label from clusters. For example:\npgo delete label mycluster --label=env=research pgo delete label all --label=env=research pgo delete label --selector=group=southwest --label=env=research  pgo delete label [flags]  Options  -h, --help help for label --label string The label to delete for any selected or specified clusters. -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo delete - Delete a backup, benchmark, cluster, pgbouncer, pgpool, label, policy, or user  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_delete_pgbouncer/",
	"title": "pgo_delete_pgbouncer",
	"tags": [],
	"description": "",
	"content": " pgo delete pgbouncer Delete a pgbouncer from a cluster\nSynopsis Delete a pgbouncer from a cluster. For example:\npgo delete pgbouncer mycluster  pgo delete pgbouncer [flags]  Options  -h, --help help for pgbouncer -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo delete - Delete a backup, benchmark, cluster, pgbouncer, pgpool, label, policy, or user  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_delete_pgpool/",
	"title": "pgo_delete_pgpool",
	"tags": [],
	"description": "",
	"content": " pgo delete pgpool Delete a pgpool from a cluster\nSynopsis Delete a pgpool from a cluster. For example:\npgo delete pgpool mycluster  pgo delete pgpool [flags]  Options  -h, --help help for pgpool -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo delete - Delete a backup, benchmark, cluster, pgbouncer, pgpool, label, policy, or user  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_delete_policy/",
	"title": "pgo_delete_policy",
	"tags": [],
	"description": "",
	"content": " pgo delete policy Delete a SQL policy\nSynopsis Delete a policy. For example:\npgo delete policy mypolicy  pgo delete policy [flags]  Options  --all all resources. -h, --help help for policy --no-prompt No command line confirmation.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo delete - Delete a backup, benchmark, cluster, pgbouncer, pgpool, label, policy, or user  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_delete_schedule/",
	"title": "pgo_delete_schedule",
	"tags": [],
	"description": "",
	"content": " pgo delete schedule Delete a schedule\nSynopsis Delete a cron-like schedule. For example:\npgo delete schedule mycluster pgo delete schedule --selector=env=test pgo delete schedule --schedule-name=mycluster-pgbackrest-full  pgo delete schedule [flags]  Options  -h, --help help for schedule --no-prompt No command line confirmation. --schedule-name string The name of the schedule to delete. -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo delete - Delete a backup, benchmark, cluster, pgbouncer, pgpool, label, policy, or user  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_delete_upgrade/",
	"title": "pgo_delete_upgrade",
	"tags": [],
	"description": "",
	"content": " pgo delete upgrade Delete an upgrade\nSynopsis Delete an upgrade. For example:\npgo delete upgrade mydatabase  pgo delete upgrade [flags]  Options  -h, --help help for upgrade  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo delete - Delete a backup, benchmark, cluster, pgbouncer, pgpool, label, policy, upgrade, or user  Auto generated by spf13/cobra on 27-Mar-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_delete_user/",
	"title": "pgo_delete_user",
	"tags": [],
	"description": "",
	"content": " pgo delete user Delete a user\nSynopsis Delete a user. For example:\npgo delete user someuser --selector=name=mycluster  pgo delete user [flags]  Options  -h, --help help for user -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo delete - Delete a backup, benchmark, cluster, pgbouncer, pgpool, label, policy, or user  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_df/",
	"title": "pgo_df",
	"tags": [],
	"description": "",
	"content": " pgo df Display disk space for clusters\nSynopsis Displays the disk status for PostgreSQL clusters. For example:\npgo df mycluster pgo df all pgo df --selector=env=research  pgo df [flags]  Options  -h, --help help for df -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_failover/",
	"title": "pgo_failover",
	"tags": [],
	"description": "",
	"content": " pgo failover Performs a manual failover\nSynopsis Performs a manual failover. For example:\npgo failover mycluster  pgo failover [flags]  Options  --autofail-replace-replica string If 'true', causes a replica to be created to replace the promoted replica. If 'false', causes a replica to not be created, if not set, the pgo.yaml AutofailReplaceReplica setting is used. -h, --help help for failover --no-prompt No command line confirmation. --query Prints the list of failover candidates. --target string The replica target which the failover will occur on.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_label/",
	"title": "pgo_label",
	"tags": [],
	"description": "",
	"content": " pgo label Label a set of clusters\nSynopsis LABEL allows you to add or remove a label on a set of clusters. For example:\npgo label mycluster yourcluster --label=environment=prod pgo label all --label=environment=prod pgo label --label=environment=prod --selector=name=mycluster pgo label --label=environment=prod --selector=status=final --dry-run  pgo label [flags]  Options  --dry-run Shows the clusters that the label would be applied to, without labelling them. -h, --help help for label --label string The new label to apply for any selected or specified clusters. -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_load/",
	"title": "pgo_load",
	"tags": [],
	"description": "",
	"content": " pgo load Perform a data load\nSynopsis LOAD performs a load. For example:\npgo load --load-config=./load.json --selector=project=xray  pgo load [flags]  Options  -h, --help help for load --load-config string The load configuration to use that defines the load job. --policies string The policies to apply before loading a file, comma separated. -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_ls/",
	"title": "pgo_ls",
	"tags": [],
	"description": "",
	"content": " pgo ls Perform a ls command on a cluster\nSynopsis LS performs a Linux ls command on a cluster directory. For example:\npgo ls mycluster /pgdata/mycluster/pg_log  pgo ls [flags]  Options  -h, --help help for ls  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_reload/",
	"title": "pgo_reload",
	"tags": [],
	"description": "",
	"content": " pgo reload Perform a cluster reload\nSynopsis RELOAD performs a PostgreSQL reload on a cluster or set of clusters. For example:\npgo reload mycluster  pgo reload [flags]  Options  -h, --help help for reload --no-prompt No command line confirmation. -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_restore/",
	"title": "pgo_restore",
	"tags": [],
	"description": "",
	"content": " pgo restore Perform a restore from previous backup\nSynopsis RESTORE performs a restore to a new PostgreSQL cluster. This includes stopping the database and recreating a new primary with the restored data. Valid backup types to restore from are pgbackrest and pgdump. For example:\npgo restore mycluster  pgo restore [flags]  Options  --backup-opts string The restore options for pgbackrest or pgdump. --backup-path string The path for the directory containing the pg_basebackup backup to be utilized for the restore. If omitted, defaults to the latest backup. --backup-pvc string The PVC containing the pgdump or pgbasebackup backup directory to restore from. --backup-type string The type of backup to restore from, default is pgbackrest. Valid types are pgbackrest, pgdump or pgbasebackup. -h, --help help for restore --no-prompt No command line confirmation. --node-label string The node label (key=value) to use when scheduling the restore job, and in the case of a pgBackRest restore, also the new (i.e. restored) primary deployment. If not set, any node is used. --pgbackrest-storage-type string The type of storage to use for a pgBackRest restore. Either \u0026quot;local\u0026quot;, \u0026quot;s3\u0026quot;. (default \u0026quot;local\u0026quot;) --pitr-target string The PITR target, being a PostgreSQL timestamp such as '2018-08-13 11:25:42.582117-04'. --restore-to-pvc string The name of the PVC to restore into when restoring from a pgbasebackup backup.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_scale/",
	"title": "pgo_scale",
	"tags": [],
	"description": "",
	"content": " pgo scale Scale a PostgreSQL cluster\nSynopsis The scale command allows you to adjust a Cluster\u0026rsquo;s replica configuration. For example:\npgo scale mycluster --replica-count=1  pgo scale [flags]  Options  --ccp-image-tag string The CCPImageTag to use for cluster creation. If specified, overrides the .pgo.yaml setting. -h, --help help for scale --no-prompt No command line confirmation. --node-label string The node label (key) to use in placing the primary database. If not set, any node is used. --replica-count int The replica count to apply to the clusters. (default 1) --resources-config string The name of a container resource configuration in pgo.yaml that holds CPU and memory requests and limits. --service-type string The service type to use in the replica Service. If not set, the default in pgo.yaml will be used. --storage-config string The name of a Storage config in pgo.yaml to use for the replica storage.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_scaledown/",
	"title": "pgo_scaledown",
	"tags": [],
	"description": "",
	"content": " pgo scaledown Scale down a PostgreSQL cluster\nSynopsis The scale command allows you to scale down a Cluster\u0026rsquo;s replica configuration. For example:\nTo list targetable replicas: pgo scaledown mycluster --query To scale down a specific replica: pgo scaledown mycluster --target=mycluster-replica-xxxx  pgo scaledown [flags]  Options  -d, --delete-data Causes the data for the scaled down replica to be removed permanently. -h, --help help for scaledown --no-prompt No command line confirmation. --query Prints the list of targetable replica candidates. --target string The replica to target for scaling down  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_show/",
	"title": "pgo_show",
	"tags": [],
	"description": "",
	"content": " pgo show Show the description of a cluster\nSynopsis Show allows you to show the details of a policy, backup, pvc, or cluster. For example:\npgo show backup mycluster pgo show backup mycluster --backup-type=pgbackrest pgo show benchmark mycluster pgo show cluster mycluster pgo show config pgo show policy policy1 pgo show pvc mycluster pgo show namespace pgo show workflow 25927091-b343-4017-be4b-71575f0b3eb5 pgo show user mycluster  pgo show [flags]  Options  -h, --help help for show  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface. pgo show backup - Show backup information pgo show benchmark - Show benchmark information pgo show cluster - Show cluster information pgo show config - Show configuration information pgo show namespace - Show namespace information pgo show policy - Show policy information pgo show pvc - Show PVC information pgo show schedule - Show schedule information pgo show user - Show user information pgo show workflow - Show workflow information  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_show_backup/",
	"title": "pgo_show_backup",
	"tags": [],
	"description": "",
	"content": " pgo show backup Show backup information\nSynopsis Show backup information. For example:\npgo show backup mycluser  pgo show backup [flags]  Options  --backup-type string The backup type output to list. Valid choices are pgbasebackup or pgbackrest (default). (default \u0026quot;pgbackrest\u0026quot;) -h, --help help for backup  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo show - Show the description of a cluster  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_show_benchmark/",
	"title": "pgo_show_benchmark",
	"tags": [],
	"description": "",
	"content": " pgo show benchmark Show benchmark information\nSynopsis Show benchmark results for clusters. For example:\npgo show benchmark mycluster pgo show benchmark --selector=pg-cluster=mycluster  pgo show benchmark [flags]  Options  -h, --help help for benchmark -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo show - Show the description of a cluster  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_show_cluster/",
	"title": "pgo_show_cluster",
	"tags": [],
	"description": "",
	"content": " pgo show cluster Show cluster information\nSynopsis Show a PostgreSQL cluster. For example:\npgo show cluster --all pgo show cluster mycluster  pgo show cluster [flags]  Options  --all show all resources. --ccp-image-tag string Filter the results based on the image tag of the cluster. -h, --help help for cluster -o, --output string The output format. Currently, json is the only supported value. -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo show - Show the description of a cluster  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_show_config/",
	"title": "pgo_show_config",
	"tags": [],
	"description": "",
	"content": " pgo show config Show configuration information\nSynopsis Show configuration information for the Operator. For example:\npgo show config  pgo show config [flags]  Options  -h, --help help for config  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo show - Show the description of a cluster  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_show_namespace/",
	"title": "pgo_show_namespace",
	"tags": [],
	"description": "",
	"content": " pgo show namespace Show namespace information\nSynopsis Show namespace information for the Operator. For example:\npgo show namespace  pgo show namespace [flags]  Options  -h, --help help for namespace  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo show - Show the description of a cluster  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_show_policy/",
	"title": "pgo_show_policy",
	"tags": [],
	"description": "",
	"content": " pgo show policy Show policy information\nSynopsis Show policy information. For example:\npgo show policy --all pgo show policy policy1  pgo show policy [flags]  Options  --all show all resources. -h, --help help for policy  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo show - Show the description of a cluster  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_show_pvc/",
	"title": "pgo_show_pvc",
	"tags": [],
	"description": "",
	"content": " pgo show pvc Show PVC information\nSynopsis Show PVC information. For example:\npgo show pvc mycluster pgo show pvc --all pgo show pvc mycluster-backup pgo show pvc mycluster-xlog pgo show pvc a2-backup --pvc-root=a2-backups/2019-01-12-17-09-42  pgo show pvc [flags]  Options  --all show all resources. -h, --help help for pvc --node-label string The node label (key=value) to use --pvc-root string The PVC directory to list.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo show - Show the description of a cluster  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_show_schedule/",
	"title": "pgo_show_schedule",
	"tags": [],
	"description": "",
	"content": " pgo show schedule Show schedule information\nSynopsis Show cron-like schedules. For example:\npgo show schedule mycluster pgo show schedule --selector=pg-cluster=mycluster pgo show schedule --schedule-name=mycluster-pgbackrest-full  pgo show schedule [flags]  Options  -h, --help help for schedule --no-prompt No command line confirmation. --schedule-name string The name of the schedule to show. -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo show - Show the description of a cluster  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_show_upgrade/",
	"title": "pgo_show_upgrade",
	"tags": [],
	"description": "",
	"content": " pgo show upgrade Show upgrade information\nSynopsis Show upgrade information. For example:\npgo show upgrade mycluster  pgo show upgrade [flags]  Options  -h, --help help for upgrade  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo show - Show the description of a cluster  Auto generated by spf13/cobra on 27-Mar-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_show_user/",
	"title": "pgo_show_user",
	"tags": [],
	"description": "",
	"content": " pgo show user Show user information\nSynopsis Show users on a cluster. For example:\npgo show user mycluster  pgo show user [flags]  Options  --expired string Shows passwords that will expire in X days. -h, --help help for user -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo show - Show the description of a cluster  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_show_workflow/",
	"title": "pgo_show_workflow",
	"tags": [],
	"description": "",
	"content": " pgo show workflow Show workflow information\nSynopsis Show workflow information for a given workflow. For example:\npgo show workflow 25927091-b343-4017-be4b-71575f0b3eb5  pgo show workflow [flags]  Options  -h, --help help for workflow  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo show - Show the description of a cluster  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_status/",
	"title": "pgo_status",
	"tags": [],
	"description": "",
	"content": " pgo status Display PostgreSQL cluster status\nSynopsis Display namespace wide information for PostgreSQL clusters. For example:\npgo status  pgo status [flags]  Options  -h, --help help for status -o, --output string The output format. Currently, json is the only supported value.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_test/",
	"title": "pgo_test",
	"tags": [],
	"description": "",
	"content": " pgo test Test cluster connectivity\nSynopsis TEST allows you to test the connectivity for a cluster. For example:\npgo test mycluster pgo test --selector=env=research pgo test --all  pgo test [flags]  Options  --all test all resources. -h, --help help for test -o, --output string The output format. Currently, json is the only supported value. -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_update/",
	"title": "pgo_update",
	"tags": [],
	"description": "",
	"content": " pgo update Update a cluster\nSynopsis The update command allows you to update a cluster. For example:\npgo update cluster mycluster --autofail=false pgo update cluster mycluster --autofail=true  pgo update [flags]  Options  -h, --help help for update  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface. pgo update cluster - Update a PostgreSQL cluster  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_update_cluster/",
	"title": "pgo_update_cluster",
	"tags": [],
	"description": "",
	"content": " pgo update cluster Update a PostgreSQL cluster\nSynopsis Update a PostgreSQL cluster. For example:\npgo update cluster all --autofail=false pgo update cluster mycluster --autofail=true  pgo update cluster [flags]  Options  --autofail string If set, will cause the autofail label on the pgcluster CRD for this cluster to be updated to either true or false, valid values are true or false. -h, --help help for cluster --no-prompt No command line confirmation. -s, --selector string The selector to use for cluster filtering.  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo update - Update a cluster  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_upgrade/",
	"title": "pgo_upgrade",
	"tags": [],
	"description": "",
	"content": " pgo upgrade Perform an upgrade\nSynopsis UPGRADE performs an upgrade on a PostgreSQL cluster. For example:\npgo upgrade mycluster\npgo upgrade [flags]  Options  --ccp-image-tag string The CCPImageTag to use for cluster creation. If specified, overrides the pgo.yaml setting. -h, --help help for upgrade  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_user/",
	"title": "pgo_user",
	"tags": [],
	"description": "",
	"content": " pgo user Manage PostgreSQL users\nSynopsis USER allows you to manage users and passwords across a set of clusters. For example:\npgo user --selector=name=mycluster --update-passwords pgo user --change-password=bob --expired=300 --selector=name=mycluster --password=newpass  pgo user [flags]  Options  --change-password string Updates the password for a user on selective clusters. --db string Grants the user access to a database. --expired string required flag when updating passwords that will expire in X days using --update-passwords flag. -h, --help help for user --password string Specifies the user password when updating a user password or creating a new user. --password-length int If no password is supplied, this is the length of the auto generated password (default 12) -s, --selector string The selector to use for cluster filtering. --update-passwords Performs password updating on expired passwords. --valid-days int Sets passwords for new users to X days. (default 30)  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
},
{
	"uri": "https://crunchydata.github.io/postgres-operator/latest/operatorcli/cli/pgo_version/",
	"title": "pgo_version",
	"tags": [],
	"description": "",
	"content": " pgo version Print version information for the PostgreSQL Operator\nSynopsis VERSION allows you to print version information for the postgres-operator. For example:\npgo version  pgo version [flags]  Options  -h, --help help for version  Options inherited from parent commands  --apiserver-url string The URL for the PostgreSQL Operator apiserver. --debug Enable debugging when true. -n, --namespace string The namespace to use for pgo requests. --pgo-ca-cert string The CA Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-cert string The Client Certificate file path for authenticating to the PostgreSQL Operator apiserver. --pgo-client-key string The Client Key file path for authenticating to the PostgreSQL Operator apiserver.  SEE ALSO  pgo - The pgo command line interface.  Auto generated by spf13/cobra on 3-Jun-2019 "
}]